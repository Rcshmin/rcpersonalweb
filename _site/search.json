[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Posts",
    "section": "",
    "text": "Same tree?\n\n\nMy solution to leetcode problem 100\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum depth of a binary tree\n\n\nMy solution to leetcode problem 104\n\n\n\nJan 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-fold cross validation from scratch\n\n\nA from scratch implementation of k-fold cross validation in R and some examples\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision tree classifier from scratch v2\n\n\nFixing my previous attempt using the OOP framework\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-nn from scratch\n\n\nA from scratch implementation of the k-nearest-neighbors algorithm in R\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision tree classifier from scratch v1\n\n\nA from scratch implementation of the decision tree classifier algorithm in R\n\n\n\nMay 7, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-01-16-leetcode-max-depth-tree/index.html",
    "href": "posts/2025-01-16-leetcode-max-depth-tree/index.html",
    "title": "Maximum depth of a binary tree",
    "section": "",
    "text": "The binary tree\nA binary tree is a type of data structure. It is pretty well defined.\n\n\n\n\n\n\nWhat is a binary tree?\n\n\n\nA binary tree data structure is a hierarchical data structure in which each node has at most two children, referred to as the left child and the right child.\n\n\nThis might be better seen through some examples.\nIn the tree below, 2 is the left child of 1, and 3 is the right child of 1. Similarly, 4 is the left child of 2 and 5 is the right child of 2. Finally, 6 is the left child of 3, and 7 is the right child of 3.\n\n\n\n\n\n\n---\ntitle: Tree1\n---\ngraph TD\n  A((01)) --&gt; B((02))\n  A((01)) --&gt; C((03))\n  B((02)) --&gt; D((04))\n  B((02)) --&gt; E((05))\n  C((03)) --&gt; F((06))\n  C((03)) --&gt; G((07))\n  \n\n\n\n\nFigure¬†1\n\n\n\n\n\nHere is another tree.\n\n\n\n\n\n\n---\ntitle: Tree2\n---\ngraph TD\n    A((08))--&gt;B((03))\n    A--&gt;C((10))\n    B--&gt;D((01))\n    B--&gt;E((06))\n    C--&gt;F((09))\n    C--&gt;G((14))\n    E--&gt;H((04))\n    E--&gt;I((07))\n  \n\n\n\n\nFigure¬†2\n\n\n\n\n\nAnd another tree which is different to the other two\n\n\n\n\n\n\n---\ntitle: Tree3\n---\ngraph TD\n    A((01))--&gt;B((02))\n    A--&gt;C((03))\n    B--&gt;D((04))\n    B--&gt;E((05))\n    C--&gt;F((06))\n    C--&gt;G((07))\n    D--&gt;H((08))\n    D--&gt;I((09))\n    F--&gt;J((10))\n    F--&gt;K((11))\n  \n\n\n\n\nFigure¬†3\n\n\n\n\n\nSo a binary tree is simply a layer of connected nodes, where each node is either the left or right child of another node. Note there can also only be one root node.\n\n\nThe tree data structure\nThe tree data structure can be implemented using the reference class system using the R6 package. To make a tree, a node is created with attributes, left, right and value. These left and right attributes are initialized as being null, while the value is a numeric.\n\nNode = R6Class(\n  classname = \"Node\", \n  public = list(\n    left = NA,\n    right = NA,\n    value = \"numeric\",\n    initialize =\n      function(left = NA, right = NA, value = NA){\n        self$left = left\n        self$right = right\n        self$value = value\n      }\n  )\n)\n\nThe idea is that by nesting nodes, a tree can be created\n\n##### Here is tree 1 from earlier #####\n\ntree1 = Node$new(value = 1)\ntree1[[\"left\"]] = Node$new(value = 2)\ntree1[[\"right\"]] = Node$new(value = 3)\ntree1[[\"left\"]][[\"left\"]] = Node$new(value = 4) \ntree1[[\"left\"]][[\"right\"]] = Node$new(value = 5)\ntree1[[\"right\"]][[\"left\"]] = Node$new(value = 6)\ntree1[[\"right\"]][[\"right\"]] = Node$new(value = 7)\n\n##### Another tree... #####\n\ntree2 = Node$new(value = 1)\ntree2[[\"left\"]] = Node$new(value = 2)\ntree2[[\"right\"]] = Node$new(value = 3)\n\n##### And another tree as well... #####\n\ntree3 = Node$new(value = 1)\ntree3[[\"left\"]] = Node$new(value = 2)\ntree3[[\"left\"]][[\"left\"]] = Node$new(value = 3) \ntree3[[\"left\"]][[\"left\"]][[\"left\"]] = Node$new(value = 4) \ntree3[[\"left\"]][[\"left\"]][[\"left\"]][[\"left\"]] = Node$new(value = 5) \n\n\n\nMaximum depth of a tree\nLeetcode defines the depth of the tree as below\n\n\n\n\n\n\nWhat is the maximum depth of a tree?\n\n\n\nA binary tree‚Äôs maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node.\n\n\nThis way of defining a tree means than a tree with a single node has a depth of 1. A null tree then has a depth of 0. The coded tree‚Äôs above have depths of 3, 2 and 5.\nI present my code for the algorithm that calculates the maximum depth.\n\nTree = R6Class(\n  classname = \"Tree\",\n  public = list(\n  depth = \"double\",\n  initialize = function(depth = -Inf){\n    self$depth = depth\n  },\n  max.depth = \n    function(tree, cdepth = 0){\n    \n    #check for null case  \n    if(is.null(tree)){\n      self$depth = 0\n      return()\n    }\n      \n    #increase current depth by 1  \n    up.depth = cdepth + 1\n    \n    #compare to global depth and increase if larger\n    if(up.depth &gt; self$depth){\n      self$depth = up.depth\n    }\n    \n    #check for nullity of children nodes\n    left.na = !is.environment(tree$left) & is.environment(tree$right)\n    right.na = is.environment(tree$left) & !is.environment(tree$right)\n    both.na = !is.environment(tree$left) & !is.environment(tree$right)\n    \n    #choose what type of recursion to carry out\n    if(left.na == TRUE){\n      self$max.depth(tree$right, cdepth = up.depth)\n    } else if(right.na == TRUE){\n      self$max.depth(tree$left, cdepth = up.depth)\n    } else if (both.na == TRUE){\n      return()\n    } else {\n      self$max.depth(tree$right, cdepth = up.depth)\n      self$max.depth(tree$left, cdepth = up.depth)\n    }\n  },\n  max_depth =\n    function(tree){\n      self$max.depth(tree)\n      return(self$depth)\n    }\n))\n\nI started by defining a class, Tree with attribute depth as double initialized as being \\(-\\infty\\). I also create two methods max.depth, which finds the max depth of a tree, and a dummy method max_depth for printing the result of the first method.\nThe first method is the primary operator; max.depth is a recursive function that works in the following way\nThe arguments of the function are a tree (to be inputted in), and depth which is initialized as 0. The function first checks for the edge case of a null tree; if a null tree is inputted, it returns the initialized depth of 0.\nThe current depth is increased by 1 to make the ‚Äòup depth‚Äô. The up depth is then compared to the global depth stored as an attribute. If it is greater, the attribute value is updated with the up depth value. If not, it is kept the same.\nThe function then checks the structure of the children node for the current tree. A binary tree can have a left node only, right node only or both children nodes. The structure is stored as several logicals indicating which of the above cases are true. A nested if statement is then used to carry out recursion depending on the child structure\nIf only a single child node exists, that being the left or right, the function is called recursively with updated input arguments on that child node. What this means is that, the input tree is updated to the tree stored in the left or right attribute of the current node. The depth fed in the recursive call is also the up depth.\nIf both children node exist, the recursive call is made on both the left and right child node, with the updated trees being fed through likewise. If the current node has no children, then the stopping condition takes place, which is to return nothing.\nIn this way, the function simply recursively descends down a tree, and compares its current depth to the global depth, updating that value when necessary.\nThe second method simply runs the first method, and prints it result.\nHere are some test cases.\n\n##### Test case 1... #####\n\ntree1 = c()\nTree$new()$max_depth(tree1)\n\n[1] 0\n\n##### Test case 2... #####\n\ntree2 = Node$new(value = 1)\nTree$new()$max_depth(tree2)\n\n[1] 1\n\n##### Test case 3... #####\n\ntree3 = Node$new(value = 1)\ntree3[[\"left\"]] = Node$new(value = 2)\ntree3[[\"right\"]] = Node$new(value = 3)\nTree$new()$max_depth(tree3)\n\n[1] 2\n\n##### Test case 4... #####\n\ntree4 = Node$new(value = 1)\ntree4[[\"left\"]] = Node$new(value = 2)\ntree4[[\"left\"]][[\"left\"]] = Node$new(value = 3) \ntree4[[\"left\"]][[\"left\"]][[\"left\"]] = Node$new(value = 4) \ntree4[[\"left\"]][[\"left\"]][[\"left\"]][[\"left\"]] = Node$new(value = 5) \nTree$new()$max_depth(tree4)\n\n[1] 5\n\n##### Test case 5... #####\n\ntree5 = Node$new(value = 1)\ntree5[[\"left\"]] = Node$new(value = 2)\ntree5[[\"right\"]] = Node$new(value = 3)\ntree5[[\"left\"]][[\"left\"]] = Node$new(value = 4) \ntree5[[\"left\"]][[\"right\"]] = Node$new(value = 5)\ntree5[[\"right\"]][[\"left\"]] = Node$new(value = 6)\ntree5[[\"right\"]][[\"right\"]] = Node$new(value = 7)\nTree$new()$max_depth(tree5)\n\n[1] 3"
  },
  {
    "objectID": "posts/2024-12-11-k-fold-cv/index.html",
    "href": "posts/2024-12-11-k-fold-cv/index.html",
    "title": "K-fold cross validation from scratch",
    "section": "",
    "text": "A key term in k-fold cross validation is the last one. As such, I would like to preface my implementation of the algorithm, with a discussion of what model validation itself is. The word validation does not have unambiguous meaning. It is the affirmation or recognition of the validity of something. On this basis, it is pretty easy to take a guess at what model validation is. Rather than slapping the definition in right here, lets first take a step back and remind ourselves of what machine learning is, as this will assist us.\n\nMachine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. The models that are constructed by machine learning algorithms are then used to provide insight.\n\nSo machine learning is all about making predictions and models learning from known data such that they are able to generalise their learning to new data. And these predictions come after various processes including data preparation, model selection, model training and parameter tuning; the umbrella term used for these processes is model development. Model validation then occurs, followed by model implementation\n\nModel validation is the set of processes and activities intended to verify that models are performing as expected\n\nIn this sense, model validation is all about checking whether the model achieve its intended purpose. Just at the name suggests, the model seeks validation. While we now know that k-fold CV is attempting to check whether the model ‚Äúperforms as expected‚Äù, our understanding of the algorithm has not gotten particularly deep. To remedy that, I will once again digress.\nThe realm of machine learning is a wide one indeed. There are many models, utilizing different approaches to attempt to imitate human learning. Broadly speaking, approaches can be divided into three groups\n\nSupervised learning: This approach is characterized by the practice of building mathematical models on a set of data that contain both inputs and the desired output. The data is commonly labelled as being training data. Each data point or row in the training data set contains features (a.k.a covariates) and an associated label. This training data set is also commonly known as the feature vector; it can be decomposed into the covariates and the supervisory signal. An algorithm is then applied to the training data, which produces an inferred function which maps each data point (covariates or predictors) to some label or value. The ultimate goal is for the algorithm to build a function or model which learns from the training data set, and is then able to classify or regress on unseen data points which it was not trained on. Some algorithms will produce a function which is already best fitted to the training data, for example logistic regression, whereas others, may require hyperparameter tuning, an example being, k-NN.\nUnsupervised learning: In this approach we provide a data set to the algorithm, but omit the labels. An algorithm of this approach then finds structure in the data (grouping and clustering). These algorithms do not respond from ‚Äòfeedback‚Äô as they do not use training data that is labeled or classified.\nReinforcement learning: ‚ÄúAn area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward‚Äù. Straight from Wikipedia."
  },
  {
    "objectID": "posts/2024-12-11-k-fold-cv/index.html#train-test-split",
    "href": "posts/2024-12-11-k-fold-cv/index.html#train-test-split",
    "title": "K-fold cross validation from scratch",
    "section": "Train-test split",
    "text": "Train-test split\n\n\n\n\n\n\n\n\n\nPhew, its good that we got that out the way. What the above chunky block of words served to illustrate is that much of machine learning is about a model learning on some data, and then predicting on new examples. Now back to the question of how can we validate/benchmark/test a model? In any context, we would definitely want to test the ‚Äúgeneralization performance‚Äù of our model before applying it. Well if a model is built using data, then it makes sense that the only way to test it is by using data. But where does that testing data come from? This is where the train-test split comes in. Rather than using all of our available data to train our model, we split the data into a training partition, upon which the algorithm learns, and a testing partition which is not trained upon and serves as that ‚Äúnew/unseen‚Äù data. These splits can be of any form \\(X-Y\\) where \\(X\\) is the percentage of the data reserved to training the model, and \\(Y\\) is the rest used for testing.\n\ntrain_test_split &lt;- function(data, test){\n  #Store data\n  t_data = data\n  #Assign amount of samples\n  if(test &lt; 1){\n    sample_test = ceiling(test*nrow(t_data))\n    sample_train = nrow(t_data) - sample_test\n  } else {\n    sample_test = test\n    sample_train = nrow(t_data) - sample_test\n  }\n  #Shuffle data \n  t_data = t_data[sample(nrow(t_data)), ]\n  row.names(t_data) = 1:nrow(t_data)\n  #Assign partition and indices\n  indices = \n    sample(x = sample(1:nrow(t_data), size = nrow(t_data), replace = FALSE))\n  primary = rep(x = c(\"train\"), sample_train)\n  secondary = rep(x = c(\"test\"), sample_test)\n  total = append(primary, secondary)\n  t_data$my.folds[indices] = total\n  return(t_data)\n}\n\nJust for the sake of example, I have shown how you can use the code in conjunction with other packages, to calculate a performance metric.\n\n#Example of using a train test split\nset.seed(23891)\nttsplit = train_test_split(data = iris, test = 0.1)\nNBclassifier = \n  naiveBayes(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, \n                          data  = subset(ttsplit, ttsplit$my.folds == \"train\"))\nttsplit1 = subset(ttsplit, ttsplit$my.folds == \"test\")[, -6]\nttsplit1$prediction = predict(NBclassifier, newdata = ttsplit1, type = \"class\")\ntable(ttsplit1$Species, ttsplit1$prediction)\n\n            \n             setosa versicolor virginica\n  setosa          3          0         0\n  versicolor      0          5         1\n  virginica       0          0         6\n\n\nAs can be seen from the confusion matrix, one versicolor was classified as virginica from the naive Bayes classifier."
  },
  {
    "objectID": "posts/2024-12-11-k-fold-cv/index.html#k-fold-cross-validation",
    "href": "posts/2024-12-11-k-fold-cv/index.html#k-fold-cross-validation",
    "title": "K-fold cross validation from scratch",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\n\n\n\n\n\n\n\n\nK-fold cross validation expands upon the concept of the train-test split. In k-fold cross validation, we first randomly split our data to \\(k\\) partitions. We then use \\(k-1\\) of those partitions to create the training set, with the remaining partition becoming the testing set. This is repeated until each partition has served as the testing set. For example, 10 fold cross validation would split the data into 10 folds; 10% of the data would be used for testing, and 90% would be used for training. Note that, as you increase \\(k\\) the size of the testing partition becomes smaller, while the training partition becomes bigger. I will discuss an extreme case of this, where \\(k\\) is the largest possible size it can be later.\nYou may be wondering why k-fold cross validation is much better than the single train-test split. There are numerous reasons.\n\nUse of all data: A simple reason for sure. In a single test-split, some samples will be used for training, while others are reserved for testing. In k-fold, all of the available will take turns in being used for training and testing. In this regards, k-fold certainly leverages whatever data one may have to a better degree\nReduced variance of the performance metric: A notable issue with the single train-test split is that there is a very large number of potential train-test splits. Each of these splits will produce a different value of the performance estimate when the trained model is tested against the testing set. K-fold cross validation will let us average the performance estimate \\(k\\) times across unique train-test splits. This is especially useful as it is possible that one of those splits could be biased, or not represent the data particularly well. Model performance could be too high or too low in that case, but the algorithm would average this out. Overall, we come out, with a more robust estimate of the chosen models performance; and especially its ability to generalize to unseen data.\nGood for limited data: If data is sparse, a single train-test split will lead to a high variance in the performance metric due to skewed split. Resampling this small data set multiple times with k-fold cross validation will alleviate this issue.\nLarge data sets: With large data sets (think millions, if not billions of rows) training the model and testing it might be too computationally expensive even for a single train-test split. It might be better to take a subset of the whole data, say 100,000 of those rows, and perform k-fold cross validation.\nHyper-parameter tuning: Many machine learning algorithms come with a slight flexibility in their design. They allow the user to a change a value, which changes how the algorithm operates. K-fold cross validation allows the user to check which value for a certain model has the best performance.\nModel comparison: If our estimate of the performance metric is now more robust via k-fold cross validation ‚Äî while a single train-test split is a snapshot ‚Äî then we are now better able compare the performance of different machine learning models\n\nK-fold cross validation is not without its disadvantages\n\nMore computationally expensive: K-fold cross validation require way more operations to perform, given the additional steps it performs in comparison to a single split. This also makes it longer to perform, as the model needs to be trained \\(k\\) times, and evaluate \\(k\\) times as well."
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html",
    "title": "K-nn from scratch",
    "section": "",
    "text": "The k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method. Non-parametric meaning the algorithm does not make any distributional assumptions about the data, and supervised meaning data with labels is used.\n\n‚ÄúThe k-nearest neighbors algorithm (k-NN) was first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors - Wikipedia‚Äù\n\nIn simple terms, the algorithm is passed a data set with labels, and a new point which is not in the provided data. The algorithm uses the proximity of other points from the data set near the new data point to make predictions about the individual point. This prediction can either be a classification, or a numeric value. Pictures always help, so lets take a look at one!\n\n\nIn the below image we have two features, namely \\(X_{1}\\) and \\(X_{2}\\). We also have two classes, A and B, which are respectively denoted by yellow and purple points. Our new point which is to be classified or regressed on is red. You might already be guessing how some of the terms such as ‚Äòproximity‚Äô and ‚Äòplurality vote‚Äô factor into this algorithm from the image, but I will spell it out for you JIC. So to assign the red point a class we look at its \\(k\\) nearest neighbors. The word neighbor implies nearness. So we look at the points that are closest to the red point. For \\(k=1\\), this is the single closes point, for \\(k=2\\) this is the two closest points, for \\(k=3\\) this is the three closest points and so on. Now, consider the \\(k=3\\) case show in the image. Of the three closest points to the red point, two are purple and one is yellow. If a ‚Äòplurality vote‚Äô was two occur, one yellow point would vote that the red point should be assigned to class A, whereas two purple points would vote that it should be class B. Clearly, the purple points win, so the red point would be assigned to class B. But this will not always be the case. With \\(k=6\\) it is direct that yellow points outnumber the purple, and hence the ‚Äòvote‚Äô is won by the yellow points; the red point will be assigned to class A.\n\n\n\n\n\n\n\n\n\n\n\n\nThere is always the chance that there will be an equal amount of the ‚Äòhighest vote number‚Äô for some \\(k\\). This means when the vote occurred, there was an equal amount of points belonging to two or more different classes (tied values). There are multiple ways to eliminate this tie.\n\nChoose a different \\(k\\): A tie will likely not exist for all potential values of \\(k\\). So we can change our \\(k\\). Simple enough. But arbitrarily choosing any other \\(k\\) does not ensure that there will be no ties\nRandomly choose between the tied values: Just as the name suggests\nAllow in until natural stop: This one is a little more nuanced than the others. Choose the smallest number \\(k\\) where \\(k \\geq 2\\) such that there exists no ties"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#visual-example",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#visual-example",
    "title": "K-nn from scratch",
    "section": "",
    "text": "In the below image we have two features, namely \\(X_{1}\\) and \\(X_{2}\\). We also have two classes, A and B, which are respectively denoted by yellow and purple points. Our new point which is to be classified or regressed on is red. You might already be guessing how some of the terms such as ‚Äòproximity‚Äô and ‚Äòplurality vote‚Äô factor into this algorithm from the image, but I will spell it out for you JIC. So to assign the red point a class we look at its \\(k\\) nearest neighbors. The word neighbor implies nearness. So we look at the points that are closest to the red point. For \\(k=1\\), this is the single closes point, for \\(k=2\\) this is the two closest points, for \\(k=3\\) this is the three closest points and so on. Now, consider the \\(k=3\\) case show in the image. Of the three closest points to the red point, two are purple and one is yellow. If a ‚Äòplurality vote‚Äô was two occur, one yellow point would vote that the red point should be assigned to class A, whereas two purple points would vote that it should be class B. Clearly, the purple points win, so the red point would be assigned to class B. But this will not always be the case. With \\(k=6\\) it is direct that yellow points outnumber the purple, and hence the ‚Äòvote‚Äô is won by the yellow points; the red point will be assigned to class A."
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#an-interesting-case",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#an-interesting-case",
    "title": "K-nn from scratch",
    "section": "",
    "text": "There is always the chance that there will be an equal amount of the ‚Äòhighest vote number‚Äô for some \\(k\\). This means when the vote occurred, there was an equal amount of points belonging to two or more different classes (tied values). There are multiple ways to eliminate this tie.\n\nChoose a different \\(k\\): A tie will likely not exist for all potential values of \\(k\\). So we can change our \\(k\\). Simple enough. But arbitrarily choosing any other \\(k\\) does not ensure that there will be no ties\nRandomly choose between the tied values: Just as the name suggests\nAllow in until natural stop: This one is a little more nuanced than the others. Choose the smallest number \\(k\\) where \\(k \\geq 2\\) such that there exists no ties"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#euclidean-distance",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#euclidean-distance",
    "title": "K-nn from scratch",
    "section": "Euclidean distance",
    "text": "Euclidean distance\nEuclidean distance is a perhaps the most common and well known measure of distance in mathematics. As I recall from my golden high school days, it is used just about everywhere; complex numbers, vectors, trigonometry, calculus and so on.\n\n‚ÄúIn mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance.‚Äù\n\n\n\n\n\n\n\n\n\n\nIn two-dimensions the Euclidean distance is simple enough. It follows directly from Pythagoras theorem, and is nothing more than the sum of the squared differences of the \\(x\\) and \\(y\\) coordinates. Suppose we have two points in the Cartesian space \\(p = (p_{1}, p_{2})\\) and \\(q = (q_{1}, q_{2})\\). Then the Euclidean distance is\n\\[\\begin{align*}\n\nd(p,q) = \\sqrt{(q_{1}-p_{1})^2 +(q_{2}-p_{2})^2}\n\n\\end{align*}\\]\nSome fancy geometric proofs, and pattern observing lands us the \\(n\\) dimensional Euclidean distance, which is not too different from the two dimensional one. Suppose we have two points in a \\(n\\) dimensional space, \\(p = (p_{1}, p_{2}, ... , p_{n})\\) and \\(q = (q_{1}, q_{2}, ... , q_{n})\\) . Then the Euclidean distance is \\[\\begin{align*}\n\nd(p,q) = \\sqrt{(q_{1}-p_{1})^2 + (q_{2}-p_{2})^2 + (q_{3}-p_{3})^2 + ... + (q_{n}-p_{n})^2}\n\n\\end{align*}\\]\nThis is the metric I will use to establish ‚Äòproximity‚Äô later due to its simplicity, and appropriateness for the latter notion. But it is important to note that the Euclidean distance does not perform as well in very high dimensions numbers. Enter, the curse of dimensionality!.\n\n‚ÄúThe curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector‚Äù\n\nJust to spice things up, I will throw another distance metric into the bag, but lets first encode the euclidean distance metric\n\nget_euclid &lt;- function(p, q){\n  #Check for same length\n  if(length(p) != length(q)) return(\"Error, unequal length!\")\n  #Calculate distance\n  distance = sqrt(sum((q-p)^2))\n  return(distance)\n}"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#cosine-distance",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#cosine-distance",
    "title": "K-nn from scratch",
    "section": "Cosine distance",
    "text": "Cosine distance\n\nCosine similarity/distance measures the similarity between two vectors of an inner product space. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. Using this distance we get values between 0 and 1, where 0 means the vectors are 100% similar to each other and 1 means they are not similar at all\n\nCosine distance has the below formula\n\\[\\begin{align*}\n\n\\cos{\\theta} = \\frac{\\overrightarrow{a}\\cdot\\overrightarrow{b}}{||\\overrightarrow{a}|| \\times ||\\overrightarrow{b}||}\n\n\\end{align*}\\]\nThose of you familiar with vectors and linear algebra will note that cosine similarity is just a re-arrangement of the dot product formula. The closer the cosine value is to 1, the smaller the angle between the two vectors, and the greater the match between the two vectors. And vice versa. Earlier I mentioned that euclidean distance breaks down in higher dimensions. Since cosine distance looks how closely two points are oriented to each other, it deals with data with a large number of dimensions better (i.e.¬†when the data is sparse)\n\nget_cosine &lt;- function(p, q){\n  #Check for same length\n  if(length(p) != length(q)) return(\"Error, unequal length!\")\n  #Calculate cosine distance\n  cos_dot = sum(p*q)\n  p_mag = sqrt(sum(p^2))\n  q_mag = sqrt(sum(q^2))\n  cos_dis = 1 - cos_dot/(p_mag*q_mag)\n  return(round(cos_dis, digits = 10))\n}"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#classification",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#classification",
    "title": "K-nn from scratch",
    "section": "Classification",
    "text": "Classification\nThe kNN algorithm for classification calculates the distance between the new point and all the other points in the data set. It sorts the distances from the shortest to furthest and then chooses the \\(k\\) smallest distances. The get_majority_vote() helper is called on the labels of theses \\(k\\) smallest distances to produce the dominant label which is then returned by the function. As it turns out the regression problem is even easier than that of classification. A majority vote helper is not even required. We simply find the \\(k\\) nearest neighbors, and average the value of the target variable to find the value for the new point.\n\nknn_classify &lt;- function(df, k, new_point, type){\n  #Check the new point is of same length\n  if(length(new_point) != ncol(df) - 1) return(\"Error, unequal length!\")\n  #Calculate the distances and order them\n  df_euclid = df[, (ncol(df) - 1):ncol(df)] \n  df_points = as.data.frame(df[, 1:(ncol(df)-1)])\n  for(i in 1:nrow(df_euclid)){\n    df_euclid[i, 1] = get_euclid(p = new_point, \n                                 q = as.numeric(df_points[i, ]))\n  }\n  df_order = df_euclid[order(df_euclid[,1]), ]\n  #Return the k-closest neighbors\n  df_knn = df_order[1:k, ]\n  labels_k = df_knn[, 2]\n  #The dominating label\n  dom_label = get_majority_vote(labels = labels_k)\n  return(dom_label)\n}"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#regression",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#regression",
    "title": "K-nn from scratch",
    "section": "Regression",
    "text": "Regression\nAs it turns out the regression problem is even easier than that of classification. A majority vote helper is not even required. We simply find the \\(k\\) nearest neighbors, and average the value of the target variable to find the value for the new point.\n\nknn_regress &lt;- function(df, k, new_point){\n  #Check the new point is of same length\n  if(length(new_point) != ncol(df) - 1) return(\"Error, unequal length!\")\n  #Calculate the distances and order them\n  df_euclid = df[, (ncol(df) - 1):ncol(df)] \n  df_points = as.data.frame(df[, 1:(ncol(df)-1)])\n  for(i in 1:nrow(df_euclid)){\n    df_euclid[i, 1] = get_euclid(p = new_point, \n                                 q = as.numeric(df_points[i, ]))\n  }\n  df_order = df_euclid[order(df_euclid[,1]), ]\n  #Return the k-closest neighbors\n  df_knn = df_order[1:k, ]\n  #The average value\n  regress_val = mean(df_knn[, 2])\n  return(regress_val)\n}\n\nknn_regress(df = iris[,1:3], k = 1, new_point = c(7, 4))\n\n[1] 6.1"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#classification-performance",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#classification-performance",
    "title": "K-nn from scratch",
    "section": "Classification performance",
    "text": "Classification performance\n\nModel validation\nOne way we can evaluate the classifier is by splitting the iris data into a training and testing set. The training set will consist of points that will be used the ‚Äúnew points‚Äù in the testing set. We can then compare the classifications outputted by using the algorithm on the testing set versus what the classifications actually are.\n\n#Train test split\nset.seed(21312)\ndt = sort(sample(nrow(iris), nrow(iris)*.7))\ntrainData &lt;- iris[dt,]\ntestData &lt;- iris[-dt,]\n\n#Accuracy of k for dif vals on one possible split\naccuracy = c()\nfor(j in 1:20){\nfor(i in 1:nrow(testData)){\ntestData[i, 6] = \n  knn_classify(df = trainData, k = j, \n               new_point = c(as.numeric(testData[i, 1:4])))\nif(testData[i, 6] == testData[i, 5]){\n  testData[i, 7] = TRUE\n} else {\n  testData[i, 7] = FALSE\n}}\naccuracy[j] = length(which(testData[, 7] == TRUE))/nrow(testData)\n}\naccuracy\n\n [1] 0.9333333 0.9333333 0.9555556 0.9555556 1.0000000 1.0000000 1.0000000\n [8] 1.0000000 0.9777778 0.9777778 0.9333333 0.9333333 0.9555556 0.9555556\n[15] 0.9333333 0.9333333 0.9555556 0.9555556 0.9333333 0.9333333\n\n#Confusion matrix for k = 5\nfor(i in 1:nrow(testData)){\ntestData[i, 6] = \n  knn_classify(df = trainData, k = 20, \n               new_point = c(as.numeric(testData[i, 1:4])))\nif(testData[i, 6] == testData[i, 5]){\n  testData[i, 7] = TRUE\n} else {\n  testData[i, 7] = FALSE\n}}\ntable(testData$Species, testData$V6) \n\n            \n             setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         14         1\n  virginica       0          2        15\n\n\nFor \\(k=20\\) the confusion matrix (mis-classifcation matrix), shows that the model performs quite well. Two virginica species were incorrectly classified as versicolor. One versicolor species was incorrectly classified as virginica. Else all seems well. It is also clear that for many different values of \\(k\\), the model has 100% accuracy. This means that it can correctly classify samples from the training data set by considering the proximity to points in the train data set. A 100% accuracy is not always good. It may indicate that the k-NN model is over fitted to the data, and will not generalise to new data points. This is closely related to the notion of the bias-variance tradeoff. I will not delve into the specifics for now, as this concept deserves its own explanation.\nBack to the problem at hand. Earlier we saw that there were many values of \\(k\\) which produced an accuracy of 100% on one particular train test split. However if we shuffled the data, or changed the train test split ratio, the model would perform differently for the same value of \\(k\\). Surely, there is a way to find out which \\(k\\) performs the best on average across different train test splits for our data? Enter k-fold cross validation.\n\n#Perform 10 fold cross validation\nk = 10\nz = 40\nset.seed(23131)\nk_fold_data = \n  mutate(iris, my.folds = sample(1:k, size = nrow(iris), replace = TRUE))\ntable(k_fold_data$my.folds)\n\n\n 1  2  3  4  5  6  7  8  9 10 \n15 19 17 17 12 17 14 14 12 13 \n\n#a = z is no of neighbors, b is which partition to use as test\nk_fold_accuracy = vector(\"list\", z)\nfor(a in 1:z){\n  for(b in 1:k){\n    #Split into train and test based off which partition to use\n    k_fold_test = k_fold_data[k_fold_data$my.folds == b, ]\n    k_fold_train = k_fold_data[k_fold_data$my.folds != b, 1:5]\n    \n    #For given a,b check if predicted matches true label\n    for(i in 1:nrow(k_fold_test)){\n      k_fold_test[i, 7] = \n        knn_classify(df = k_fold_train, \n                     k = a, \n                     new_point = c(as.numeric(k_fold_test[i, 1:4])))\n     if(k_fold_test[i, 5] == k_fold_test[i, 7]){\n       k_fold_test[i, 8] = TRUE\n     } else {\n        k_fold_test[i, 8] = FALSE\n      }\n    }\n    \n    #Fill list with accuracy\n    k_fold_accuracy[[a]][b] =\n      length(which(k_fold_test[, 8] == TRUE))/nrow(k_fold_test)\n    next(b)\n  }\n  next(a)\n}\n\n#Weights of each partition, then multiply to get average\nk_fold_weights = as.numeric(table(k_fold_data$my.folds)/nrow(k_fold_data))\nfor(i in 1:length(k_fold_accuracy)){\n  k_fold_accuracy[[i]] = sum(k_fold_accuracy[[i]]*k_fold_weights)\n}\n\nas.numeric(k_fold_accuracy)\n\n [1] 0.9600000 0.9600000 0.9666667 0.9666667 0.9600000 0.9600000 0.9666667\n [8] 0.9666667 0.9600000 0.9600000 0.9666667 0.9666667 0.9800000 0.9800000\n[15] 0.9800000 0.9800000 0.9800000 0.9800000 0.9666667 0.9666667 0.9800000\n[22] 0.9800000 0.9733333 0.9733333 0.9733333 0.9733333 0.9600000 0.9600000\n[29] 0.9533333 0.9533333 0.9533333 0.9533333 0.9600000 0.9600000 0.9666667\n[36] 0.9666667 0.9600000 0.9600000 0.9533333 0.9533333\n\n\nAverage model performance never reaches 100% accuracy for any \\(k\\). As can seen above the highest average accuracy attained by the model is 98%, which occurs for several different \\(k\\) values. Overall, if we said that all \\(k\\)‚Äôs of a reasonably small magnitude perform similarly, we would not be wrong.\n\n#Format data for error\nk_fold_plot = as.data.frame(t(as.data.frame(k_fold_accuracy)))\nrow.names(k_fold_plot) = 1:nrow(k_fold_plot)\nk_fold_plot$k = 1:nrow(k_fold_plot)\ncolnames(k_fold_plot) = c(\"Error (%)\", \"k\")\nk_fold_plot$Error = (1 - k_fold_plot$Error)*100 \n\n#Create line plot\nggplot(data = k_fold_plot, aes(x = k, y = Error)) +\n  geom_line(color = \"orange\", size = 1.5) +\n    theme_classic() +\n  labs(title = \"Cross validated error rate by differing k's\", \n       subtitle = \"Which k performs the best on average?\") +\n    theme(text=element_text(family=\"montsr\"), \n        plot.title = element_text(size=32),\n        plot.subtitle = element_text(size=22),\n        axis.title = element_text(size=22),\n        axis.text = element_text(size=16),\n        legend.text = element_text(size=16), \n        legend.title = element_text(size=22)) +\n  scale_color_pilot()\n\n\n\n\n\n\n\n\n\n\nReason for good performance\nPerhaps, the reason why \\(k\\) does not appear to significantly affect model performance lies in the structure of the iris data set.\n\n#Pair-wise plot\nggpairs(data = iris, columns = c(\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"),\n  lower = list(continuous = \"points\", mapping = aes(color = Species)), \n  upper = list(continuous = \"points\", mapping = aes(color = Species)), \n  diag = list(continuous = \"barDiag\", mapping = aes(color = Species)))\n\n\n\n\n\n\n\n\nOn every pairwise combination of the data, there appears to be significant grouping of the different classes. This shows that the iris data is well suited to the k-NN algorithm. Given that the grouping is quite significant, this confirms the relative indifference of the accuracy of the algorithm to different values of \\(k\\). However, the above plot is only a visualization of pairwise combinations in two dimensions. It would be nice if there were a way to visualize the grouping of classes for all four features of the data.\n\n#Format data for plot\niris_id = iris\niris_id[, 6] = 1:nrow(iris)\niris_parallel_plot = iris_id %&gt;%  \n  pivot_longer(cols = c(\"Sepal.Length\", \"Sepal.Width\", \n                        \"Petal.Length\", \"Petal.Width\"), \n              names_to = \"Feature\", values_to = \"Feature.Value\")\n\n#Make plot\nggplot(data = iris_parallel_plot, \n       aes(x = Feature, y = Feature.Value, color = Species, group = V6)) +\n  geom_point() + \n  geom_line() + \n  scale_x_discrete(limits = c(\"Sepal.Length\", \n                              \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")) +\n  theme_classic() +\n  labs(title = \"Parallel lines plot of iris\", \n       subtitle = \"Is iris well suited for the k-NN classification\n       algorithm?\") +\n    theme(text=element_text(family=\"montsr\"), \n        plot.title = element_text(size=32),\n        plot.subtitle = element_text(size=22),\n        axis.title = element_text(size=22),\n        axis.text = element_text(size=16),\n        legend.text = element_text(size=16), \n        legend.title = element_text(size=22)) +\n  scale_color_pilot()\n\n\n\n\n\n\n\n\nThe above is a visualization of the multivariate data, known as a parallel lines plot. Since we are searching for a reason for good model performance with the k-NN algorithm, we are looking for grouping in the plot. What I mean by grouping, is that different observations (rows) on iris for a given species follow a similar trajectory across all features; i.e.¬†they are nearby to each other. Given grouping holds relatively well, one thing in addition to consider is distinctness of the trajectories for each species. In the above plot, the setosa species has distinct shape. Versicolor and virginica are similar. This could also be seen earlier in the pair plots.\n\nIn data visualization, an Andrews plot or Andrews curve is a way to visualize structure in high-dimensional data. It is basically a rolled-down, non-integer version of the Kent‚ÄìKiviat radar m chart, or a smoothed version of a parallel coordinate plot. It is named after the statistician David F. Andrews.A value \\(x\\) is a high-dimensional datapoint if it is an element of \\(\\mathbb{R}^{d}\\). We can represent high-dimensional data with a number for each of their dimensions, \\(x=\\{x_{1},x_{2},\\ldots ,x_{d}\\}\\). To visualize them, the Andrews plot defines a finite Fourier series:\\(f_{x}(t)={\\frac {x_{1}}{\\sqrt{2}}}+x_{2}\\sin(t)+x_{3}\\cos(t)+x_{4}\\sin(2t)+x_{5}\\cos(2t)+\\cdots\\)\n\n\n\n\n\n\n\n\n\n\nThe Andrews Curves for iris are the smoothed out versions of the parallel lines plot. For iris, they confirm what we saw earlier regarding the grouping of classes."
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#regression-performance",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#regression-performance",
    "title": "K-nn from scratch",
    "section": "Regression performance",
    "text": "Regression performance\nOur discussion of the performance of k-NN in regression for different values of \\(k\\), will center around the calculation of the cross validated mean squared error (MSE).\n\nThe mean squared error (MSE) of an estiamtor measures the average square of the errors; the average squared difference between the stimated values and the actual value. It is strictly positive, and represents the quality of an estimator. MSE decreases as the error approaches zero\n\nMSE has the following formula; \\(n\\) is the number of observations, \\(Y_{i}\\) is the \\(i\\)th observed value of the variable, \\(\\hat{Y}_{i}\\) is the \\(i\\)th predicted value of the variable\n\\[\\begin{align*}\n\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n}{(Y_{i} - \\hat{Y_{i}})^2}\n\n\\end{align*}\\]\nIn addition to this, I will also calculate mean absolute percentage error (MAPE) and mean absolute error (MAE). MAE gives how far on average, each predicted value is from the true value in distance. MAPE is the same as MAE but a percentage\n\\[\\begin{align*}\n\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n}{|Y_{i} - \\hat{Y_{i}}|}\n\n\\end{align*}\\] \\[\\begin{align*}\n\n\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n}{\\frac{|Y_{i} - \\hat{Y_{i}}|}{Y_{i}}}\n\n\\end{align*}\\]\nTo calculate regression performance, I will use the first three variables of the iris data to estimate the fourth variable\n\n#Perform 10 fold cross validation\nk = 10\nz = 40\nset.seed(23321)\nk_fold_data = mutate(iris, my.folds = sample(1:k, size = nrow(iris), \n                                             replace = TRUE))\ntable(k_fold_data$my.folds)\n\n\n 1  2  3  4  5  6  7  8  9 10 \n17 12 16 14 10 16 14 15 21 15 \n\n#a = z is no of neighbors, b is which partition to use as test\nk_fold_accuracy = vector(\"list\", z)\nk_fold_accuracy1 = vector(\"list\", z)\nk_fold_accuracy2 = vector(\"list\", z)\nfor(a in 1:z){\n  for(b in 1:k){\n    #Split into train and test based off which partition to use\n    k_fold_test = k_fold_data[k_fold_data$my.folds == b, 1:4]\n    k_fold_train = k_fold_data[k_fold_data$my.folds != b, 1:4]\n    \n    #For given a,b check if predicted matches true label\n    for(i in 1:nrow(k_fold_test)){\n      k_fold_test[i, 5] = \n        knn_regress(df = k_fold_train,\n                      k = a, \n                        new_point = as.numeric(k_fold_test[i, 1:3]))\n      k_fold_test[i, 6] = \n        (k_fold_test[i, 4] - k_fold_test[i, 5])^2\n      k_fold_test[i, 7] = \n        abs(k_fold_test[i, 4] - k_fold_test[i, 5])\n      k_fold_test[i, 8] = \n        abs(k_fold_test[i, 4] - k_fold_test[i, 5])/k_fold_test[i, 4]\n    }\n    #Fill list with accuracy\n    k_fold_accuracy[[a]][b] = (1/nrow(k_fold_test))*sum(k_fold_test[, 6])\n    k_fold_accuracy1[[a]][b] = (1/nrow(k_fold_test))*sum(k_fold_test[, 7])\n    k_fold_accuracy2[[a]][b] = (1/nrow(k_fold_test))*sum(k_fold_test[, 8])\n    next(b)\n  }\n  next(a)\n}\n\n#Weights of each partition, then multiply to get average\nk_fold_weights = as.numeric(table(k_fold_data$my.folds)/nrow(k_fold_data))\nfor(i in 1:length(k_fold_accuracy)){\n  k_fold_accuracy[[i]] = sum(k_fold_accuracy[[i]]*k_fold_weights)\n  k_fold_accuracy1[[i]] = sum(k_fold_accuracy1[[i]]*k_fold_weights)\n  k_fold_accuracy2[[i]] = sum(k_fold_accuracy2[[i]]*k_fold_weights)\n}\n\n#Making three line plots\nk_fold_mse = as.data.frame(t(as.data.frame(k_fold_accuracy)))\nrow.names(k_fold_mse) = 1:nrow(k_fold_mse)\ncolnames(k_fold_mse) = c(\"MSE\")\nk_fold_mse$k = 1:nrow(k_fold_mse)\n\nk_fold_mae = as.data.frame(t(as.data.frame(k_fold_accuracy1)))\nrow.names(k_fold_mae) = 1:nrow(k_fold_mae)\ncolnames(k_fold_mae) = c(\"MAE\")\nk_fold_mae$k = 1:nrow(k_fold_mae)\n\nk_fold_mape = as.data.frame(t(as.data.frame(k_fold_accuracy2)))\nrow.names(k_fold_mape) = 1:nrow(k_fold_mape)\ncolnames(k_fold_mape) = c(\"MAPE\")\nk_fold_mape$k = 1:nrow(k_fold_mape)\nk_fold_mape$MAPE = k_fold_mape$MAPE * 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn terms of MSE, it seems that at \\(k=6\\), MSE is the closes to zero. However, in terms of the MAE and MAPE, \\(k=10\\) seems to be the best. This is as for \\(k=10\\) the k-NN regressor is on average 0.14 units away from the true value, or around 19% away. One thing is clear though. The k-NN algorithm performs much better in the classification problem, then regression on the iris data set."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rashmin Chitale",
    "section": "",
    "text": "Who am I?\n\n\n\nMy name is Rashmin Chitale.\n\n\n\n\n\n\n\n\nWhat is this website?\n\n\n\nIt is my personal website.\n\n\n\n\n\n\n\n\nWhat is the purpose of this website?\n\n\n\nOn this website, you will find some projects and other code related endeavors I have worked on in my downtime.\n\n\n\n\n\n\n\n\nWhat are your interests?\n\n\n\nSome of my interests include mathematics, statistics, data science, cricketüèèand fishing .\n\n\n\n\n\n\n\n\nWhat is your educational background?\n\n\n\nI am a recent university graduate who studied a Bachelor of Actuarial Studies with a Bachelor of Applied Finance."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html",
    "title": "Decision tree classifier from scratch v1",
    "section": "",
    "text": "Decision trees are a non-parametric supervised learning method. Supervised means that the input and output data is labelled. Non-parametric means that no assumptions are made regarding the assumptions of the population. This definition is obviously not that useful, but with some further consideration we can make some sense of it. Analogous to a tree in real life, a decision tree is a tree-like model of decisions. In essence, we pass this model data on several input variables, and ask it to create a tree that predicts the value of the target variable. This is all probably best understood through an example, so let‚Äôs take a look at one‚Ä¶\n\n\nIn this example we consider how we can classify a fish as a salmon or tuna based off its length and weight. Looking at the below data set, it is easy to see some patterns, in how each type of fish is grouped based off its two characteristics. For instance, any fish with a length less than around 2.5 is a tuna. Continuing on, a fish with length greater than 2.5, and weight less than around 4 is a salmon. We could also say that a fish with length greater than 7.2 and weight greater than 4.1 is a tuna. You are probably getting the gist of it at this point‚Ä¶\n\n\n\n\n\n\n\n\n\nIf we take all of these splits, or otherwise decisions and organised them, it would look something like this\n\n\nn= 1000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 1000 392 tuna (0.3920000 0.6080000)  \n    2) length&gt;=2.996015 693 301 salmon (0.5656566 0.4343434)  \n      4) weight&lt; 4.006707 306   0 salmon (1.0000000 0.0000000) *\n      5) weight&gt;=4.006707 387  86 tuna (0.2222222 0.7777778)  \n       10) length&lt; 6.978225 220  86 tuna (0.3909091 0.6090909)  \n         20) length&lt; 3.398175 19   2 salmon (0.8947368 0.1052632) *\n         21) length&gt;=3.398175 201  69 tuna (0.3432836 0.6567164)  \n           42) length&lt; 4.998481 86  41 tuna (0.4767442 0.5232558)  \n             84) weight&gt;=6.948427 41   0 salmon (1.0000000 0.0000000) *\n             85) weight&lt; 6.948427 45   0 tuna (0.0000000 1.0000000) *\n           43) length&gt;=4.998481 115  28 tuna (0.2434783 0.7565217)  \n             86) length&gt;=6.017146 58  28 tuna (0.4827586 0.5172414)  \n              172) weight&lt; 6.832503 28   0 salmon (1.0000000 0.0000000) *\n              173) weight&gt;=6.832503 30   0 tuna (0.0000000 1.0000000) *\n             87) length&lt; 6.017146 57   0 tuna (0.0000000 1.0000000) *\n       11) length&gt;=6.978225 167   0 tuna (0.0000000 1.0000000) *\n    3) length&lt; 2.996015 307   0 tuna (0.0000000 1.0000000) *\n\n\n\n\n\n\n\n\n\nAnd voila, we have our tree! From referring to the diagram above, we can see that the decision tree has created splits across our input variables (weight and length) such that our scatterplot becomes partitioned into rectangular regions containing each type of the fish (the target variable). This is exactly what the decision tree classifier does. Some of you may be asking why this is useful? Well suppose we have a noob fisherman in the Atlantic ocean (which only contains salmon and tuna for our purposes) who has just caught his first fish, but is unsure of what fish it is. Taking the decision tree just produced, this noob fisherman could then identify what fish he has caught. Well this is not exactly the most realistic scenario, but you get the idea hopefully. So now that we know what the algorithm does, lets take a look at some terms that are often thrown around when talking about decisions tree‚Äôs."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#example",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#example",
    "title": "Decision tree classifier from scratch v1",
    "section": "",
    "text": "In this example we consider how we can classify a fish as a salmon or tuna based off its length and weight. Looking at the below data set, it is easy to see some patterns, in how each type of fish is grouped based off its two characteristics. For instance, any fish with a length less than around 2.5 is a tuna. Continuing on, a fish with length greater than 2.5, and weight less than around 4 is a salmon. We could also say that a fish with length greater than 7.2 and weight greater than 4.1 is a tuna. You are probably getting the gist of it at this point‚Ä¶\n\n\n\n\n\n\n\n\n\nIf we take all of these splits, or otherwise decisions and organised them, it would look something like this\n\n\nn= 1000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 1000 392 tuna (0.3920000 0.6080000)  \n    2) length&gt;=2.996015 693 301 salmon (0.5656566 0.4343434)  \n      4) weight&lt; 4.006707 306   0 salmon (1.0000000 0.0000000) *\n      5) weight&gt;=4.006707 387  86 tuna (0.2222222 0.7777778)  \n       10) length&lt; 6.978225 220  86 tuna (0.3909091 0.6090909)  \n         20) length&lt; 3.398175 19   2 salmon (0.8947368 0.1052632) *\n         21) length&gt;=3.398175 201  69 tuna (0.3432836 0.6567164)  \n           42) length&lt; 4.998481 86  41 tuna (0.4767442 0.5232558)  \n             84) weight&gt;=6.948427 41   0 salmon (1.0000000 0.0000000) *\n             85) weight&lt; 6.948427 45   0 tuna (0.0000000 1.0000000) *\n           43) length&gt;=4.998481 115  28 tuna (0.2434783 0.7565217)  \n             86) length&gt;=6.017146 58  28 tuna (0.4827586 0.5172414)  \n              172) weight&lt; 6.832503 28   0 salmon (1.0000000 0.0000000) *\n              173) weight&gt;=6.832503 30   0 tuna (0.0000000 1.0000000) *\n             87) length&lt; 6.017146 57   0 tuna (0.0000000 1.0000000) *\n       11) length&gt;=6.978225 167   0 tuna (0.0000000 1.0000000) *\n    3) length&lt; 2.996015 307   0 tuna (0.0000000 1.0000000) *\n\n\n\n\n\n\n\n\n\nAnd voila, we have our tree! From referring to the diagram above, we can see that the decision tree has created splits across our input variables (weight and length) such that our scatterplot becomes partitioned into rectangular regions containing each type of the fish (the target variable). This is exactly what the decision tree classifier does. Some of you may be asking why this is useful? Well suppose we have a noob fisherman in the Atlantic ocean (which only contains salmon and tuna for our purposes) who has just caught his first fish, but is unsure of what fish it is. Taking the decision tree just produced, this noob fisherman could then identify what fish he has caught. Well this is not exactly the most realistic scenario, but you get the idea hopefully. So now that we know what the algorithm does, lets take a look at some terms that are often thrown around when talking about decisions tree‚Äôs."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#entropy",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#entropy",
    "title": "Decision tree classifier from scratch v1",
    "section": "Entropy",
    "text": "Entropy\nEntropy quantifies the amount of uncertainty involved in the outcome of a process. It has formula\n\\[\\begin{align*}\n\\mbox{Entropy} &= \\sum_{c}{f_{c} \\cdot I(c)}\n\\end{align*}\\]\nwhere \\(f_{c}\\) is the fraction of a class in data set and \\(I(c)\\) is the information content in the class. Also \\(c\\) is the total number of classes. In the context of decision tree classifiers, \\(I(c) = -\\log_{2}{(f_{c})}\\) which gives\n\\[\\begin{align*}\n\\mbox{Entropy} &= -\\sum_{c}{f_{c} \\cdot \\log_{2}{(f_{c})}} & \\\\\n\\end{align*}\\]\nThe choice of the \\(\\text{log}\\) function is beyond the scope of this article, but those interested may wish to take a look at this article. Implementing an entropy function can be done as shown below\n\nget_entropy &lt;- function(x){\n  #Assume x is factor of labels\n  if(length(x) == 0) return(0)\n  weights = table(x)/length(x)\n  info_content = -weights*log2(weights)\n  entropy = sum(info_content)\n  return(entropy)\n}\n\nWe can perform a few checks using our function to check that it performs as expected\n\n#Entropy is zero?\nget_entropy(c(0,0,0,0,0))\n## [1] 0\n#Entropy is one?\nget_entropy(c(0,0,0,1,1,1))\n## [1] 1\n#Entropy is non-zero?\nget_entropy(salmon_fish$type)\n## [1] 0.9660781\n\nIf we only have one class then our data is homogeneous, which means there is no uncertainty regarding the data. If we have an equal number of observations across two classes, then uncertainty is at its maximum. Note that a lower value of entropy always means less uncertainty. A simple situation which may help one understand how entropy works is the flipping of a coin\n\n\n\n\n\n\n\n\n\nFor the coin flip (two classes), entropy is constrained between zero and one. A fair coin has the most uncertainty, whereas a coin with some bias towards one side has less uncertainty. This intuitively makes sense."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#gini-impurity",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#gini-impurity",
    "title": "Decision tree classifier from scratch v1",
    "section": "Gini impurity",
    "text": "Gini impurity\nGini impurity is one of the other available measures for calculating uncertainty. While entropy does not have an intuitive interpretation of its formula, we can say that gini impurity calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly precisely. It has formula\n\\[\\begin{align*}\n\\mbox{Gini index} &= 1 - \\sum_{i=1}^{n}{(p_{i})^2}\n\\end{align*}\\]\nwhere \\(p_{i}\\) is the probability of an element being classified for a distinct class. This can also be easily implemented\n\nget_gini_impurity &lt;- function(x){\n  #Assume x is a factor with labels\n  if(length(x) == 0) return(0)\n  weights = table(x)/length(x)\n  weights_squared = weights^2\n  sum_of_squares = sum(weights_squared)\n  gini = 1 - sum_of_squares\n  return(gini)\n}\n\nAs with entropy, we can also perform some checks\n\n#Minimum uncertainty is 0\nget_gini_impurity(c(0,0,0,0,0))\n## [1] 0\n#Maximum uncertainty is 0.5 for two classes\nget_gini_impurity(c(0,0,1,1))\n## [1] 0.5\n#Between 0.5 and 1?\nget_gini_impurity(c(1,1,2,2,3,3,4,4))\n## [1] 0.75\n\nGini impurity in the case of two classes is constrained between zero and half, with zero being minimum uncertainty and half being maximum uncertainty. However with more than two classes, the measure will always be in between zero and one. This is in contrast to entropy which has no upper bound. Once again, note that higher values of gini impurity represent greater uncertainty and vice versa."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#information-gain",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#information-gain",
    "title": "Decision tree classifier from scratch v1",
    "section": "Information gain",
    "text": "Information gain\nInformation gain serves an extension to the calculation of entropy. It is the difference in entropy between a parent node and the average entropy of its children.\n\\[\\begin{align*}\n\\overbrace{\\mbox{IG}(T,a)}^{\\mbox{information gain}} &= \\overbrace{H(T)}^{\\mbox{entropy of parent}} - \\overbrace{H(T|a)}^{\\mbox{average entropy of children}} & \\\\\n\\end{align*}\\]\nWhile we seek to minimize entropy, we alternatively seek to maximize information gain. Or in other words, we seek to find the split with the most information gain.\n\nget_information_gain &lt;- function(parent, l_child, r_child, mode = \"entropy\"){\n  #Get weights in each child\n  l_child = as.data.frame(l_child)\n  r_child = as.data.frame(r_child)\n  weight_l = nrow(l_child)/nrow(parent)\n  weight_r = nrow(r_child)/nrow(parent)\n  #Choose mode\n  if(mode == \"gini\"){\n    gain = get_gini_impurity(parent[, ncol(parent)]) - (weight_l*get_gini_impurity(l_child[, ncol(l_child)]) + weight_r*get_gini_impurity(r_child[, ncol(r_child)]))\n  } else {\n    gain = get_entropy(as.character(parent[, ncol(parent)])) - (weight_l*get_entropy(as.character(l_child[, ncol(l_child)])) + weight_r*get_entropy(as.character(r_child[, ncol(r_child)])))\n  }\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#train-test-split",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#train-test-split",
    "title": "Decision tree classifier from scratch v1",
    "section": "Train-test split",
    "text": "Train-test split\nWe will first code function to split our data into a training data set, which will be used to train the decision tree, and then a testing data set, which will be used to test how well the decision tree performs. Note that this function is not a helper that will be called by the main algorithm, but I could not find a better section to put this under.\n\nget_train_test &lt;- function(df, train_size){\n  observations = 1:nrow(df)\n  #Option for a proportion or number\n  if(train_size &lt; 1){\n    test_size_f = round(train_size*nrow(df))\n  } else {\n    test_size_f = train_size\n  }\n  #Get index of train values\n  train_index = sample(observations, size = test_size_f)\n  test_observations = nrow(df) - length(train_index)\n  #Get index of test values\n  test_index = double(length = length(observations))\n  for(i in 1:length(observations)){\n    if(any(observations[i] == train_index)){\n      next(i)\n    } else {\n      test_index[i] = observations[i]\n    }\n  }\n  test_index_f = c(subset(test_index, test_index &gt; 0))\n  #Create df's from index values\n  train_df = df[train_index, ]\n  test_df = df[test_index_f, ]\n  return(list(train_df, test_df))\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#checking-the-purity",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#checking-the-purity",
    "title": "Decision tree classifier from scratch v1",
    "section": "Checking the purity",
    "text": "Checking the purity\nThis helper will be used to check whether a subset of the original data is pure. As discussed before, a pure node is a point at which the subset of the original data contains only one class. If we find that the data is pure, we would not want to continue splitting the data, as entropy would be zero at that point. In other words, we would have full certainty over the class of the points in that node.\n\ncheck_purity &lt;- function(data){\n  #Get unique labels\n  labels = length(unique(pull(data[, -1])))\n  #Check if there is only one\n  ifelse(labels == 1, return(TRUE), return(FALSE))\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#classification",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#classification",
    "title": "Decision tree classifier from scratch v1",
    "section": "Classification",
    "text": "Classification\nAfter we have decided to stop creating new split at some point of our tree, most likely when a stopping condition is reached, we need to return a classification for the points in whichever subset of the original data we have at the node. If the data is pure, then our choice of what classification to make is rather simple. If the data is not pure, we will use the class that appears the most among the data points.\n\nclassify_data &lt;- function(data){\n  #Get labels\n  get_labels = pull(data[, -1])\n  #Get label frequency and max\n  label_freq = table(get_labels)\n  label_freq_a = as.data.frame(label_freq)\n  label_dom = max(label_freq)\n  #Get classification\n  for(i in 1:nrow(label_freq_a)){\n    if(label_freq_a$Freq[i] == label_dom){\n      classification = as.character(label_freq_a$get_labels[i])\n    } else {\n      next(i)\n    }\n  }\n  return(classification)\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#splitting-the-data",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#splitting-the-data",
    "title": "Decision tree classifier from scratch v1",
    "section": "Splitting the data",
    "text": "Splitting the data\nI am going to, un-intuitively, make the helper that will split the data before making the helper for the potential splits. Once we have a potential split value for a given feature we need to separate the parent node into two children. Anything above the value (\\(&gt;\\)) will be coined as the right node, and anything below the value (\\(\\leq\\)) will be termed the left node. These two nodes, together, are the children of the parent node.\n\nsplit_data &lt;- function(data, split_column, split_value){\n  split_c = data[[split_column]]\n  #Filter the data into above and below\n  data_below = data[split_c &lt;= split_value, ]\n  data_above = data[split_c &gt; split_value, ]\n  return(list(data_above, data_below))\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#potential-and-best-splits",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#potential-and-best-splits",
    "title": "Decision tree classifier from scratch v1",
    "section": "Potential and best splits",
    "text": "Potential and best splits\nThe potential split helper(s) are arguably the most important helper. These helpers, as the name suggests, will search our data for the split that provides the most certainty regarding the classes of the child nodes. It will return a feature, and a value for that feature for which we must split. First and foremost, we need to consider the manner in which we will search our data. There are several ways to approach the search stage. One such way is to increment through the range of a feature by a learning rate; at each of these increment, we will calculate the entropy of a split made at that point. The effectiveness of this approach is largely determined by the learning rate. A very small learning rate will take a long time iterate through the data, but will be more accurate. The converse is also true for a large learning rate. Another approach would be to only have potential splits be made on each real value a feature has. The middle ground between these approaches is to check for a potential splits in the middle of two values for a given features. I will use the third approach as it is the easiest to implement\n\nget_potential_splits &lt;- function(data){\n  #Sorting stage\n  data = data\n  col_n = ncol(data) - 1\n  for(i in 1:col_n){\n    data_i = sort(data[, i])\n    data[, i] = data_i\n  }\n  #Creating the splits\n  dat = data[0, ]\n  for(j in 1:col_n){\n    for(i in 2:nrow(data)){\n      curr_val = data[i, j]\n      previous_val = data[(i-1), j]\n      potential_val = (curr_val + previous_val)/2\n      dat[(i-1), j] = potential_val\n    }\n  }\n  dat[nrow(dat)+1, ] = data[nrow(data), ]\n  dat = dat[, 1:col_n]\n  potential_splits = as.data.frame(dat)\n  if(ncol(potential_splits) == 1){\n    colnames(potential_splits)[[1]] = colnames(data)[[1]]\n  }\n  return(potential_splits)\n}\n\n\ncalculate_overall_entropy &lt;- function(data_below, data_above){\n  #Proportion of samples in left and right children\n  n = nrow(data_below) + nrow(data_above)\n  p_data_below = nrow(data_below)/n\n  p_data_above = nrow(data_above)/n\n  #Calculate overall entropy\n  overall_entropy = \n    ((p_data_below*get_entropy(as.character(pull(data_below[, -1])))) \n     + (p_data_above*get_entropy(as.character(pull(data_above[, -1])))))\n  return(overall_entropy)\n}\n\n\ndetermine_best_split &lt;- function(data, potential_splits){\n  #Initialize overall entropy and col \n  running_entropy = 9999\n  best_split_value = 0\n  best_split_column = \"\"\n  #Find best entropy over potential splits\n  for(j in 1:ncol(potential_splits)){\n    for(i in unique(potential_splits[, j])){\n      mask_val = i \n      mask_col = j\n      splits = \n        split_data(data = data, split_column = mask_col, split_value = mask_val)\n      relative_entropy = \n        calculate_overall_entropy(data_above = splits[[1]], \n                                  data_below = splits[[2]])\n      if(relative_entropy &lt; running_entropy){\n        running_entropy = relative_entropy\n        best_split_value = mask_val\n        best_split_column = colnames(potential_splits)[j]\n      } else {\n        next(i)\n      }\n    }\n  }\n  return(list(best_split_column, best_split_value))\n}\n\n\ndetermine_best_split &lt;- function(data, potential_splits, mode = \"gini\"){\n  #Initialize overall entropy and col \n  running_gain = -Inf\n  best_split_value = 0\n  best_split_column = \"\"\n  #Find best entropy over potential splits\n  for(j in 1:ncol(potential_splits)){\n    for(i in unique(potential_splits[, j])){\n      mask_val = i \n      mask_col = j\n      splits = \n        split_data(data = data, split_column = mask_col, split_value = mask_val)\n      relative_gain =\n        get_information_gain(parent = data, r_child = splits[[1]], \n                             l_child = splits[[2]], mode = mode)\n      if(relative_gain &gt; running_gain){\n        running_gain = relative_gain\n        best_split_value = mask_val\n        best_split_column = colnames(potential_splits)[j]\n      } else {\n        next(i)\n      }\n    }\n  }\n  return(list(best_split_column, best_split_value))\n}\n\n#might need to come back in future and add as.character() to gini as well"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#recursive-function",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#recursive-function",
    "title": "Decision tree classifier from scratch v1",
    "section": "Recursive function",
    "text": "Recursive function\nThe main algorithm is a recursive function that calls the helpers to split the data, given that the stopping conditions have not been violated. In the below function, the stopping conditions are first checked. There are three conditions implemented. Namely, whether the data is fully pure, whether there is enough data points to create a new splitting node, and whether the maximum depth of the tree has been reached. The function then uses the helpers that were created earlier to recursively split the data, generating a ‚Äòyes‚Äô and ‚Äòno‚Äô answer. It prints all of this information as it does it; more nuanced code would likely build and print the tree in this same function, but I was unable to build the necessary code to do so\n\ndecision_tree_algorithm &lt;- function(df, \n                                    counter = 1, \n                                    min_samples, \n                                    max_depth, is.child = \"root\"){\n  data = df\n  #Check whether stopping conditions have been violated\n  if(any(check_purity(data), \n         nrow(data) &lt; min_samples, \n         (counter - 1) == max_depth)){\n    classification = classify_data(data)\n    return(print(paste(classification, is.child, counter)))\n  } else {\n   #Recursive part\n    \n    #Helper functions\n    potential_splits = get_potential_splits(data)\n    split_g = determine_best_split(data, potential_splits)\n    split_column = split_g[[1]]\n    split_value = split_g[[2]]\n    data_g = split_data(data, split_column, split_value)\n    data_above = data_g[[1]]\n    data_below = data_g[[2]]\n    print(paste(split_column, split_value, is.child, counter))\n    #Find the answers\n    yes_answer =\n      decision_tree_algorithm(df = data_below, \n                              counter = counter + 1, \n                              min_samples, max_depth, is.child = \"yes &lt;=\")\n    no_answer =\n      decision_tree_algorithm(df = data_above, \n                              counter = counter + 1, \n                              min_samples, max_depth, is.child = \"no &gt;\")\n    \n  }\n  \n}\n\nSo the main algorithm yields the following output on the salmon-fish data‚Ä¶\n\ndecision_tree_algorithm(df = salmon_fish, min_samples = 1, max_depth = 3)\n\n[1] \"length 2.99601479397271 root 1\"\n[1] \"tuna yes &lt;= 2\"\n[1] \"weight 4.00670743566802 no &gt; 2\"\n[1] \"salmon yes &lt;= 3\"\n[1] \"length 6.97822538046186 no &gt; 3\"\n[1] \"tuna yes &lt;= 4\"\n[1] \"tuna no &gt; 4\"\n\n\nNow this is obviously not what a decision tree looks like (as was shown earlier). But it is a step in the right direction. There are all the essential components of a decision tree in the jumble of output, but it requires sorting to be more comprehensible."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#organising-function",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#organising-function",
    "title": "Decision tree classifier from scratch v1",
    "section": "Organising function",
    "text": "Organising function\nThis function calls the decision tree recursive algorithm, captures the output and re-formats in into a data frame. An efficient implementation will likely not need this function, but since I was unable to fully incorporate the whole process in the recursive function, here we are.\n\ndecision_tree &lt;- function(df, min_samples, max_depth){\n  \n  #Store decisions and reformat\n  decisions =\n    capture.output(decision_tree_algorithm(df = df, \n                                           min_samples = min_samples, \n                                           max_depth = max_depth), append = F)\n  decisions_df = strsplit(decisions, \" \")\n  decisions_cl = sapply(decisions_df, function(x) gsub(\"\\\"\", \"\", x))\n  \n  #Make list of equal length\n  for(i in 1:length(decisions_cl)){\n  if(length(decisions_cl[[i]]) != 6){\n    if(i == 1){\n      decisions_cl[[i]] = append(decisions_cl[[i]], NA, after = 4)\n    } else {\n      decisions_cl[[i]] = append(decisions_cl[[i]], NA, after = 2)\n    }} else { next(i) }}\n  \n  #Convert to df and reformat again\n  decisions_df = as.data.frame(decisions_cl)\n  decisions_t = as.data.frame(t(decisions_df))\n  decisions_x = decisions_t[, -1]\n  row.names(decisions_x) = 1:nrow(decisions_x)\n  colnames(decisions_x) = c(\"node\", \"split.val\", \"is.child\", \"split\", \"depth\")\n  \n  #Sort the df\n  decisions_x = decisions_x[order(decisions_x$depth), ]\n  \n  return(decisions_x)\n}\n\nThe organising function returns output that looks like this\n\nsalmon_df = decision_tree(df = salmon_fish, min_samples = 1, max_depth = 2)\n\n\n\n\n\n\n\n\n\n\nThis is just what we got from the recursive function, but re-organised into a data frame format. Once again this is not in the tree format, but a rather a tabular view of each node in the tree. However if we compare our data frame on the right above to a decision tree created using rpart, it is clear that they are essentially the same. Ideally I would have wanted to somehow take the data frame that is returned from the organising function, and somehow printed it out in a tree format in the console. But that is a piece of the puzzle that I am yet to solve for now."
  },
  {
    "objectID": "posts/2025-01-13-decision-tree-classifier-oop/index.html",
    "href": "posts/2025-01-13-decision-tree-classifier-oop/index.html",
    "title": "Decision tree classifier from scratch v2",
    "section": "",
    "text": "This was my second attempt at creating a decision tree classifier. When I last attempted the problem of creating a decision tree classifier from scratch, my final solution was very incomplete. I was able to create a so called ‚Äúrecursive main function‚Äù, that would print all the nodes and store them in a data frame. However, I was unable to convert these splits into a tree like format, paralleling that of those found in the rpart and tree libraries. My solution was also unable to make a prediction on a new data point, rather, it stopped at learning on the tree. So in essence, what I had made, was not even complete in the slightest sense. A model that cannot be tested on new unseen data, is essentially an useless model in the realm of machine learning.\nThere were a few key reasons why I was unable to get to a good working solution. Firstly, there were a lot of variables and information that had to be kept tracked of. As the main recursive function generates the nodes, it was hard to store them in a tidy way, that could later be used to print the tree, and even have another function descend down it to get a new prediction. This stemmed in part from my poor understanding of how recursion worked on binary trees at the time. More so, my functional programming skills had hit a roadblock for this problem. I was also unsure whether the main recursive splitting function was working as it should, largely due to the inability to view the output of the function in a meaningful ‚Äútree-like‚Äù way. Looking back, all of these problems stemmed from one global issue ‚Äî not having an adequate data structure. I was trying to create a decision tree using a data frame representation, when in actuality the best way to represent a tree, is just a tree.\nObject Oriented Programming (OOP) turned out to be the best solution to this problem.\n\nObject-oriented programming (OOP) is a programming paradigm based on the concept of ‚Äúobjects‚Äù, which can contain data and code. The data is in the form of fields (often known as attributes or properties), and the code is in the form of procedures (often known as methods). A common feature of objects is that procedures (or methods) are attached to them and can access and modify the object‚Äôs data fields. In this brand of OOP, there is usually a special name such as this or self used to refer to the current object. In OOP, computer programs are designed by making them out of objects that interact with one another\n\nIn laymans terms OOP differs from procedural programming, in that it places more emphasis on the structure of the data\n\nMost OOP lanuages define a class. A class is an abstract blueprint that creates more specific, concrete objects. Classes often represent broad categories, like Car or Dog that share attributes. These classes define what attributes an instance of this type will have, like color, but not the value of those attributes for a specific object. Classes can also contain functions called methods that are available only to objects of that type. These functions are defined within the class and perform some action helpful to that specific object type\n\nOnce again what this means is that we can create a blueprint for a data type, and then call specific functions on that data. These function can access and modify the attributes of this new data type (most of the time!). I have shown a simple example of this below for ‚Äúrectangles‚Äù. Rectangles is a not a data type that is default implemented in most languages like vectors or lists. But it is easy to define a rectangle geometrically, so let us make a special data type for rectangles (a class). We will then create a special function to act on this new data type; it will get the area.\n\nRectangles = \n  setRefClass(Class = \"rectangles\",\n              fields = list(x = \"numeric\", y = \"numeric\"), \n              methods = \n                list(get_area = \n                       function(){\n                             return(.self$x*.self$y)\n                           },\n                     get_squared_area = \n                       function(){\n                             r_area = .self$get_area()\n                             return(.self$r_area^2)\n                           },\n                     get_cubed_area = \n                       function(){\n                             r_area = .self$get_area()\n                             return(r_area^3)\n                           }\n                         ))\n\nr1 = Rectangles(x = 5, y = 4)\nr1$get_area()\n## [1] 20\nr1$get_cubed_area()\n## [1] 8000\n\nSo OOP is all about grouping related functions and variables together. Rather than having the functions and variables be made in isolation, which is what procedural programming focuses on, lets make them together. This was the technique which I needed to finish the problem of creating a decision tree classifier from scratch. Lastly there are a few more things I want to note about OOP. Programmers and computer science courses will often reference the ‚Äúfour pillars‚Äù behind OOP. Here is what they are.\n\nInheritance: Child classes inherit data and behaviors from the parent class\nEncapsulation: Containing information in an object, exposing only selected information\nAbstraction: Only exposing high-level public methods for accessing an object\nPolymorphism: Many methods can do the same task\n\nR has several different ways of doing OOP, each using the four pillars to a different degree. These OOP systems are called S3, S4 and R6 (extension to RC). There are different trade off between using different systems. For those interested you may take a look at Hadley Wickham‚Äôs Advanced R Book (it is a free bookdown). I will be using the R6 system, as it is the easiest, and most similar to other modern systems in data science geared languages."
  },
  {
    "objectID": "posts/2025-01-13-decision-tree-classifier-oop/index.html#salmon-tuna-dataset",
    "href": "posts/2025-01-13-decision-tree-classifier-oop/index.html#salmon-tuna-dataset",
    "title": "Decision tree classifier from scratch v2",
    "section": "Salmon-tuna dataset",
    "text": "Salmon-tuna dataset\nWe begin by first creating a train test split\n\nset.seed(2310)\nsalmon_fish_split = train_test_split(data = salmon_fish, test = 0.1)\nsalmon_fish_train = subset(salmon_fish_split, salmon_fish_split$my.folds == \"train\")[, -4]\nsalmon_fish_test = subset(salmon_fish_split, salmon_fish_split$my.folds == \"test\")[, -4]\n\nWe first specify the parameters for our new tree by creating an instance of the Tree class\n\ndtree = Tree$new(min_samples = 10, \n                 max_depth = 5, \n                 mode = \"entropy\", \n                 df = salmon_fish_train)\n\nWe then can build the tree according to these parameters. Lets run the fit method as well in anticipation for making a prediction after. We will also print three\n\n#Build tree for viewing\nbtree = dtree$build_tree()\n#Store for predictions\ndtree$fit()\n#Print the tree\ndtree$print_tree()\n\n0 0 length &lt;= 2.996015 \n1   1 predict: tuna \n1   1 weight &lt;= 4.006707 \n2       2 predict: salmon \n2       2 length &lt;= 6.978225 \n3           3 length &lt;= 4.998481 \n4               4 weight &lt;= 6.948427 \n5                   5 predict: tuna \n5                   5 predict: salmon \n4               4 length &lt;= 6.017146 \n5                   5 predict: tuna \n5                   5 predict: salmon \n3           3 predict: tuna \n\n\nWe can make a prediction on a single value. If you remember the graph from my v1 version of the salmon-fish df, you can visually confirm that the function works correctly.\n\ndtree$make_prediction(y = c(7.5, 2.5))\n\n[1] \"salmon\"\n\n\nWe can then also make a prediction on the training set as well and calculate the accuracy for this particular train test split\n\npredictions = salmon_fish_test$type == dtree$get_predictions(Y = salmon_fish_test)\naccuracy = length(which(predictions == TRUE))/nrow(salmon_fish_test)\nprint(accuracy)\n\n[1] 0.94\n\n\nThe accuracy for this particular split is 94%."
  },
  {
    "objectID": "posts/2025-01-13-decision-tree-classifier-oop/index.html#iris-dataset",
    "href": "posts/2025-01-13-decision-tree-classifier-oop/index.html#iris-dataset",
    "title": "Decision tree classifier from scratch v2",
    "section": "Iris dataset",
    "text": "Iris dataset\nBelow is an example for the iris data set. I have changed the parameters of the tree as well this time.\n\n#Train test split\nset.seed(2310)\niris_split = train_test_split(data = iris, test = 0.1)\niris_train = subset(iris_split, iris_split$my.folds == \"train\")[, -6]\niris_test = subset(iris_split, iris_split$my.folds == \"test\")[, -6]\n\n#Create instance \nitree = Tree$new(min_samples = 7, \n                 max_depth = 7, \n                 mode = \"gini\", \n                 df = iris_train)\n\n#Build tree for viewing\njtree = dtree$build_tree()\n#Store for predictions\nitree$fit()\n#Print the tree\nitree$print_tree()\n\n0 0 Petal.Length &lt;= 1.9 \n1   1 predict: setosa \n1   1 Petal.Width &lt;= 1.7 \n2       2 Petal.Length &lt;= 4.9 \n3           3 Petal.Width &lt;= 1.65 \n4               4 predict: versicolor \n4               4 predict: virginica \n3           3 predict: virginica \n2       2 Petal.Length &lt;= 4.8 \n3           3 predict: virginica \n3           3 predict: virginica \n\n#Accuracy on testing data\npredictions = iris_test$Species == itree$get_predictions(Y = iris_test)\naccuracy = length(which(predictions == TRUE))/nrow(iris_test)\nprint(accuracy)\n\n[1] 1\n\n\nThe accuracy for this particular split is 100%."
  },
  {
    "objectID": "posts/2025-01-13-decision-tree-classifier-oop/index.html#single-variable-example",
    "href": "posts/2025-01-13-decision-tree-classifier-oop/index.html#single-variable-example",
    "title": "Decision tree classifier from scratch v2",
    "section": "Single variable example",
    "text": "Single variable example\nBelow, I would just like to highlight that this implementation works on data set with a single predictor only.\n\nztree = Tree$new(min_samples = 7, \n                 max_depth = 6, \n                 mode = \"entropy\", \n                 df = iris[, c(3,5)])\nltree = ztree$build_tree()\nztree$fit()\nztree$print_tree()\n\n0 0 Petal.Length &lt;= 1.9 \n1   1 predict: setosa \n1   1 Petal.Length &lt;= 4.7 \n2       2 Petal.Length &lt;= 4.4 \n3           3 predict: versicolor \n3           3 Petal.Length &lt;= 4.5 \n4               4 Petal.Length &lt;= 4.5 \n5                   5 Petal.Length &lt;= 4.5 \n6                       6 predict: versicolor \n6                       6 predict: virginica \n5                   5 predict: virginica \n4               4 predict: versicolor \n2       2 Petal.Length &lt;= 5.1 \n3           3 Petal.Length &lt;= 4.9 \n4               4 Petal.Length &lt;= 4.8 \n5                   5 predict: virginica \n5                   5 predict: virginica \n4               4 Petal.Length &lt;= 5 \n5                   5 predict: virginica \n5                   5 Petal.Length &lt;= 5.1 \n6                       6 predict: virginica \n6                       6 predict: virginica \n3           3 predict: virginica"
  },
  {
    "objectID": "posts/2025-04-22-leetcode-same-tree/index.html",
    "href": "posts/2025-04-22-leetcode-same-tree/index.html",
    "title": "Same tree?",
    "section": "",
    "text": "The background\n\n\n\n\n\n\nWhen are two trees the same?\n\n\n\nTwo trees are the same when they share exactly the same structure. This includes everything from the depth, the positions of the nodes, and the values of the nodes themselves.\n\n\nA simple example where 2 trees are exactly the same is when\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n  A((01)) --&gt; B((02))\n  A((01)) --&gt; C((03))\n  B((02)) --&gt; D((04))\n  B((02)) --&gt; E((05))\n  C((03)) --&gt; F((06))\n  C((03)) --&gt; G((07))\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n  A((01)) --&gt; B((02))\n  A((01)) --&gt; C((03))\n  B((02)) --&gt; D((04))\n  B((02)) --&gt; E((05))\n  C((03)) --&gt; F((06))\n  C((03)) --&gt; G((07))\n  \n\n\n\n\n\n\n\n\n\nversus an example when 2 trees are not the same is\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n  A((01)) --&gt; B((02))\n  A((01)) --&gt; C((03))\n  B((02)) --&gt; D((04))\n  B((02)) --&gt; E((05))\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n  A((01)) --&gt; B((02))\n  A((01)) --&gt; C((03))\n  C((03)) --&gt; D((04))\n  C((03)) --&gt; E((05))\n\n  \n\n\n\n\n\n\n\n\n\nClearly, visually it is quite simple to observe whether a tree is the same or not, but how can this be coded.\n\n\nThe solution\nIn my solution, I will carry over some of the code from my other post. I first define a node as follows\n\nNode = R6Class(\n  classname = \"Node\", \n  public = list(\n    left = NA,\n    right = NA,\n    value = \"numeric\",\n    initialize =\n      function(left = NA, right = NA, value = NA){\n        self$left = left\n        self$right = right\n        self$value = value\n      }\n  )\n)\n\nThe main algorithm is then as follows\n\nTree = R6Class(\n  classname = \"Tree\",\n  public = list(\n    initialize = function(){},\n    same.tree = \n      function(tree1, tree2){\n        \n        #check for nullity of children nodes for both trees\n        left.na1 = !is.environment(tree1$left) & is.environment(tree1$right)\n        right.na1 = is.environment(tree1$left) & !is.environment(tree1$right)\n        both.na1 = !is.environment(tree1$left) & !is.environment(tree1$right)\n        left.na2 = !is.environment(tree2$left) & is.environment(tree2$right)\n        right.na2 = is.environment(tree2$left) & !is.environment(tree2$right)\n        both.na2 = !is.environment(tree2$left) & !is.environment(tree2$right)\n        \n        #are these conditions the same across both trees\n        left.eq = left.na1 == left.na2\n        right.eq = right.na1 == right.na2\n        both.eq = both.na1 == both.na2\n        all.eq = all(left.eq, right.eq, both.eq)\n        \n        if(tree1$value == tree2$value & all.eq == TRUE){\n          #choose what type of recursion to carry out\n          if(left.na1 == TRUE){\n            self$same.tree(tree1$right, tree2$right)\n          } else if(right.na1 == TRUE){\n            self$same.tree(tree1$left, tree2$left)\n          } else if (both.na1 == TRUE){\n            #no children for both trees at current node, and value are same so...\n            return(TRUE)\n          } else {\n            self$same.tree(tree1$left, tree2$left)\n            self$same.tree(tree1$right, tree2$right)\n          }\n        } else {\n          #trees are not the same\n          return(FALSE)\n        }\n      }\n  ))\n\nThe logic behind checking whether two trees are the same as follows\n\nStarting at the root node of both trees, we check the nullity of the children node of the root node for both trees. That is, if they exist and the structure (left, right or both)\nIf this structure is the same across both trees, AND the value of the root nodes is the same as well, we assume the tree is the same thus far at the current level. If not, then the trees are different.\nWe then recursively descend down into the children node (if they exist, if they do not the trees are the same since it passed the previous checks) of both trees and repeat the above checks.\n\n\n##### Test case 1... #####\n\ntree1 = Node$new(value = 1)\ntree2 = Node$new(value = 1)\n\nTree$new()$same.tree(tree1,tree2)\n\n[1] TRUE\n\n##### Test case 2... #####\n\ntree3 = Node$new(value = 1)\ntree3[[\"left\"]] = Node$new(value = 2)\ntree3[[\"right\"]] = Node$new(value = 3)\n\nTree$new()$same.tree(tree1,tree3)\n\n[1] FALSE\n\n##### Test case 3... #####\n\ntree4 = Node$new(value = 1)\ntree4[[\"left\"]] = Node$new(value = 2)\ntree4[[\"left\"]][[\"left\"]] = Node$new(value = 3) \ntree4[[\"left\"]][[\"left\"]][[\"left\"]] = Node$new(value = 4) \ntree4[[\"left\"]][[\"left\"]][[\"left\"]][[\"left\"]] = Node$new(value = 5) \n\nTree$new()$same.tree(tree3,tree4)\n\n[1] FALSE\n\n##### Test case 4... #####\n\ntree5 = Node$new(value = 5)\ntree5[[\"left\"]] = Node$new(value = 4)\ntree5[[\"left\"]][[\"left\"]] = Node$new(value = 3) \ntree5[[\"left\"]][[\"left\"]][[\"left\"]] = Node$new(value = 2) \ntree5[[\"left\"]][[\"left\"]][[\"left\"]][[\"left\"]] = Node$new(value = 1) \n\nTree$new()$same.tree(tree4,tree5)\n\n[1] FALSE\n\n##### Test case 5... #####\n\ntree6 = Node$new(value = 1000)\ntree6[[\"left\"]] = Node$new(value = 9999)\ntree6[[\"left\"]][[\"right\"]] = Node$new(value = 991239)\ntree6[[\"left\"]][[\"right\"]][[\"left\"]] = Node$new(value = 19)\n                          \ntree7 = Node$new(value = 1000)\ntree7[[\"left\"]] = Node$new(value = 9999)\ntree7[[\"left\"]][[\"right\"]] = Node$new(value = 991239)\ntree7[[\"left\"]][[\"right\"]][[\"left\"]] = Node$new(value = 19)\n                           \nTree$new()$same.tree(tree6,tree7)\n\n[1] TRUE"
  }
]