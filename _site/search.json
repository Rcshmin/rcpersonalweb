[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "K-fold cross validation from scratch\n\n\nA from scratch implementation of k-fold cross validation in R and some examples\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-nn from scratch\n\n\nA from scratch implementation of the k-nearest-neighbors algorithm in R\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision tree classifier from scratch v1\n\n\nA from scratch implementation of the decision tree classifier algorithm in R\n\n\n\nMay 7, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html",
    "title": "Decision tree classifier from scratch v1",
    "section": "",
    "text": "Decision trees are a non-parametric supervised learning method. Supervised means that the input and output data is labelled. Non-parametric means that no assumptions are made regarding the assumptions of the population. This definition is obviously not that useful, but with some further consideration we can make some sense of it. Analogous to a tree in real life, a decision tree is a tree-like model of decisions. In essence, we pass this model data on several input variables, and ask it to create a tree that predicts the value of the target variable. This is all probably best understood through an example, so let‚Äôs take a look at one‚Ä¶\n\n\nIn this example we consider how we can classify a fish as a salmon or tuna based off its length and weight. Looking at the below data set, it is easy to see some patterns, in how each type of fish is grouped based off its two characteristics. For instance, any fish with a length less than around 2.5 is a tuna. Continuing on, a fish with length greater than 2.5, and weight less than around 4 is a salmon. We could also say that a fish with length greater than 7.2 and weight greater than 4.1 is a tuna. You are probably getting the gist of it at this point‚Ä¶\n\n\n\n\n\n\n\n\n\nIf we take all of these splits, or otherwise decisions and organised them, it would look something like this\n\n\nn= 1000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 1000 392 tuna (0.3920000 0.6080000)  \n    2) length&gt;=2.996015 693 301 salmon (0.5656566 0.4343434)  \n      4) weight&lt; 4.006707 306   0 salmon (1.0000000 0.0000000) *\n      5) weight&gt;=4.006707 387  86 tuna (0.2222222 0.7777778)  \n       10) length&lt; 6.978225 220  86 tuna (0.3909091 0.6090909)  \n         20) length&lt; 3.398175 19   2 salmon (0.8947368 0.1052632) *\n         21) length&gt;=3.398175 201  69 tuna (0.3432836 0.6567164)  \n           42) length&lt; 4.998481 86  41 tuna (0.4767442 0.5232558)  \n             84) weight&gt;=6.948427 41   0 salmon (1.0000000 0.0000000) *\n             85) weight&lt; 6.948427 45   0 tuna (0.0000000 1.0000000) *\n           43) length&gt;=4.998481 115  28 tuna (0.2434783 0.7565217)  \n             86) length&gt;=6.017146 58  28 tuna (0.4827586 0.5172414)  \n              172) weight&lt; 6.832503 28   0 salmon (1.0000000 0.0000000) *\n              173) weight&gt;=6.832503 30   0 tuna (0.0000000 1.0000000) *\n             87) length&lt; 6.017146 57   0 tuna (0.0000000 1.0000000) *\n       11) length&gt;=6.978225 167   0 tuna (0.0000000 1.0000000) *\n    3) length&lt; 2.996015 307   0 tuna (0.0000000 1.0000000) *\n\n\n\n\n\n\n\n\n\nAnd voila, we have our tree! From referring to the diagram above, we can see that the decision tree has created splits across our input variables (weight and length) such that our scatterplot becomes partitioned into rectangular regions containing each type of the fish (the target variable). This is exactly what the decision tree classifier does. Some of you may be asking why this is useful? Well suppose we have a noob fisherman in the Atlantic ocean (which only contains salmon and tuna for our purposes) who has just caught his first fish, but is unsure of what fish it is. Taking the decision tree just produced, this noob fisherman could then identify what fish he has caught. Well this is not exactly the most realistic scenario, but you get the idea hopefully. So now that we know what the algorithm does, lets take a look at some terms that are often thrown around when talking about decisions tree‚Äôs."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#example",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#example",
    "title": "Decision tree classifier from scratch v1",
    "section": "",
    "text": "In this example we consider how we can classify a fish as a salmon or tuna based off its length and weight. Looking at the below data set, it is easy to see some patterns, in how each type of fish is grouped based off its two characteristics. For instance, any fish with a length less than around 2.5 is a tuna. Continuing on, a fish with length greater than 2.5, and weight less than around 4 is a salmon. We could also say that a fish with length greater than 7.2 and weight greater than 4.1 is a tuna. You are probably getting the gist of it at this point‚Ä¶\n\n\n\n\n\n\n\n\n\nIf we take all of these splits, or otherwise decisions and organised them, it would look something like this\n\n\nn= 1000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 1000 392 tuna (0.3920000 0.6080000)  \n    2) length&gt;=2.996015 693 301 salmon (0.5656566 0.4343434)  \n      4) weight&lt; 4.006707 306   0 salmon (1.0000000 0.0000000) *\n      5) weight&gt;=4.006707 387  86 tuna (0.2222222 0.7777778)  \n       10) length&lt; 6.978225 220  86 tuna (0.3909091 0.6090909)  \n         20) length&lt; 3.398175 19   2 salmon (0.8947368 0.1052632) *\n         21) length&gt;=3.398175 201  69 tuna (0.3432836 0.6567164)  \n           42) length&lt; 4.998481 86  41 tuna (0.4767442 0.5232558)  \n             84) weight&gt;=6.948427 41   0 salmon (1.0000000 0.0000000) *\n             85) weight&lt; 6.948427 45   0 tuna (0.0000000 1.0000000) *\n           43) length&gt;=4.998481 115  28 tuna (0.2434783 0.7565217)  \n             86) length&gt;=6.017146 58  28 tuna (0.4827586 0.5172414)  \n              172) weight&lt; 6.832503 28   0 salmon (1.0000000 0.0000000) *\n              173) weight&gt;=6.832503 30   0 tuna (0.0000000 1.0000000) *\n             87) length&lt; 6.017146 57   0 tuna (0.0000000 1.0000000) *\n       11) length&gt;=6.978225 167   0 tuna (0.0000000 1.0000000) *\n    3) length&lt; 2.996015 307   0 tuna (0.0000000 1.0000000) *\n\n\n\n\n\n\n\n\n\nAnd voila, we have our tree! From referring to the diagram above, we can see that the decision tree has created splits across our input variables (weight and length) such that our scatterplot becomes partitioned into rectangular regions containing each type of the fish (the target variable). This is exactly what the decision tree classifier does. Some of you may be asking why this is useful? Well suppose we have a noob fisherman in the Atlantic ocean (which only contains salmon and tuna for our purposes) who has just caught his first fish, but is unsure of what fish it is. Taking the decision tree just produced, this noob fisherman could then identify what fish he has caught. Well this is not exactly the most realistic scenario, but you get the idea hopefully. So now that we know what the algorithm does, lets take a look at some terms that are often thrown around when talking about decisions tree‚Äôs."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#entropy",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#entropy",
    "title": "Decision tree classifier from scratch v1",
    "section": "Entropy",
    "text": "Entropy\nEntropy quantifies the amount of uncertainty involved in the outcome of a process. It has formula\n\\[\\begin{align*}\n\\mbox{Entropy} &= \\sum_{c}{f_{c} \\cdot I(c)}\n\\end{align*}\\]\nwhere \\(f_{c}\\) is the fraction of a class in data set and \\(I(c)\\) is the information content in the class. Also \\(c\\) is the total number of classes. In the context of decision tree classifiers, \\(I(c) = -\\log_{2}{(f_{c})}\\) which gives\n\\[\\begin{align*}\n\\mbox{Entropy} &= -\\sum_{c}{f_{c} \\cdot \\log_{2}{(f_{c})}} & \\\\\n\\end{align*}\\]\nThe choice of the \\(\\text{log}\\) function is beyond the scope of this article, but those interested may wish to take a look at this article. Implementing an entropy function can be done as shown below\n\nget_entropy &lt;- function(x){\n  #Assume x is factor of labels\n  if(length(x) == 0) return(0)\n  weights = table(x)/length(x)\n  info_content = -weights*log2(weights)\n  entropy = sum(info_content)\n  return(entropy)\n}\n\nWe can perform a few checks using our function to check that it performs as expected\n\n#Entropy is zero?\nget_entropy(c(0,0,0,0,0))\n## [1] 0\n#Entropy is one?\nget_entropy(c(0,0,0,1,1,1))\n## [1] 1\n#Entropy is non-zero?\nget_entropy(salmon_fish$type)\n## [1] 0.9660781\n\nIf we only have one class then our data is homogeneous, which means there is no uncertainty regarding the data. If we have an equal number of observations across two classes, then uncertainty is at its maximum. Note that a lower value of entropy always means less uncertainty. A simple situation which may help one understand how entropy works is the flipping of a coin\n\n\n\n\n\n\n\n\n\nFor the coin flip (two classes), entropy is constrained between zero and one. A fair coin has the most uncertainty, whereas a coin with some bias towards one side has less uncertainty. This intuitively makes sense."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#gini-impurity",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#gini-impurity",
    "title": "Decision tree classifier from scratch v1",
    "section": "Gini impurity",
    "text": "Gini impurity\nGini impurity is one of the other available measures for calculating uncertainty. While entropy does not have an intuitive interpretation of its formula, we can say that gini impurity calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly precisely. It has formula\n\\[\\begin{align*}\n\\mbox{Gini index} &= 1 - \\sum_{i=1}^{n}{(p_{i})^2}\n\\end{align*}\\]\nwhere \\(p_{i}\\) is the probability of an element being classified for a distinct class. This can also be easily implemented\n\nget_gini_impurity &lt;- function(x){\n  #Assume x is a factor with labels\n  if(length(x) == 0) return(0)\n  weights = table(x)/length(x)\n  weights_squared = weights^2\n  sum_of_squares = sum(weights_squared)\n  gini = 1 - sum_of_squares\n  return(gini)\n}\n\nAs with entropy, we can also perform some checks\n\n#Minimum uncertainty is 0\nget_gini_impurity(c(0,0,0,0,0))\n## [1] 0\n#Maximum uncertainty is 0.5 for two classes\nget_gini_impurity(c(0,0,1,1))\n## [1] 0.5\n#Between 0.5 and 1?\nget_gini_impurity(c(1,1,2,2,3,3,4,4))\n## [1] 0.75\n\nGini impurity in the case of two classes is constrained between zero and half, with zero being minimum uncertainty and half being maximum uncertainty. However with more than two classes, the measure will always be in between zero and one. This is in contrast to entropy which has no upper bound. Once again, note that higher values of gini impurity represent greater uncertainty and vice versa."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#information-gain",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#information-gain",
    "title": "Decision tree classifier from scratch v1",
    "section": "Information gain",
    "text": "Information gain\nInformation gain serves an extension to the calculation of entropy. It is the difference in entropy between a parent node and the average entropy of its children.\n\\[\\begin{align*}\n\\overbrace{\\mbox{IG}(T,a)}^{\\mbox{information gain}} &= \\overbrace{H(T)}^{\\mbox{entropy of parent}} - \\overbrace{H(T|a)}^{\\mbox{average entropy of children}} & \\\\\n\\end{align*}\\]\nWhile we seek to minimize entropy, we alternatively seek to maximize information gain. Or in other words, we seek to find the split with the most information gain.\n\nget_information_gain &lt;- function(parent, l_child, r_child, mode = \"entropy\"){\n  #Get weights in each child\n  l_child = as.data.frame(l_child)\n  r_child = as.data.frame(r_child)\n  weight_l = nrow(l_child)/nrow(parent)\n  weight_r = nrow(r_child)/nrow(parent)\n  #Choose mode\n  if(mode == \"gini\"){\n    gain = get_gini_impurity(parent[, ncol(parent)]) - (weight_l*get_gini_impurity(l_child[, ncol(l_child)]) + weight_r*get_gini_impurity(r_child[, ncol(r_child)]))\n  } else {\n    gain = get_entropy(as.character(parent[, ncol(parent)])) - (weight_l*get_entropy(as.character(l_child[, ncol(l_child)])) + weight_r*get_entropy(as.character(r_child[, ncol(r_child)])))\n  }\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#train-test-split",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#train-test-split",
    "title": "Decision tree classifier from scratch v1",
    "section": "Train-test split",
    "text": "Train-test split\nWe will first code function to split our data into a training data set, which will be used to train the decision tree, and then a testing data set, which will be used to test how well the decision tree performs. Note that this function is not a helper that will be called by the main algorithm, but I could not find a better section to put this under.\n\nget_train_test &lt;- function(df, train_size){\n  observations = 1:nrow(df)\n  #Option for a proportion or number\n  if(train_size &lt; 1){\n    test_size_f = round(train_size*nrow(df))\n  } else {\n    test_size_f = train_size\n  }\n  #Get index of train values\n  train_index = sample(observations, size = test_size_f)\n  test_observations = nrow(df) - length(train_index)\n  #Get index of test values\n  test_index = double(length = length(observations))\n  for(i in 1:length(observations)){\n    if(any(observations[i] == train_index)){\n      next(i)\n    } else {\n      test_index[i] = observations[i]\n    }\n  }\n  test_index_f = c(subset(test_index, test_index &gt; 0))\n  #Create df's from index values\n  train_df = df[train_index, ]\n  test_df = df[test_index_f, ]\n  return(list(train_df, test_df))\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#checking-the-purity",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#checking-the-purity",
    "title": "Decision tree classifier from scratch v1",
    "section": "Checking the purity",
    "text": "Checking the purity\nThis helper will be used to check whether a subset of the original data is pure. As discussed before, a pure node is a point at which the subset of the original data contains only one class. If we find that the data is pure, we would not want to continue splitting the data, as entropy would be zero at that point. In other words, we would have full certainty over the class of the points in that node.\n\ncheck_purity &lt;- function(data){\n  #Get unique labels\n  labels = length(unique(pull(data[, -1])))\n  #Check if there is only one\n  ifelse(labels == 1, return(TRUE), return(FALSE))\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#classification",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#classification",
    "title": "Decision tree classifier from scratch v1",
    "section": "Classification",
    "text": "Classification\nAfter we have decided to stop creating new split at some point of our tree, most likely when a stopping condition is reached, we need to return a classification for the points in whichever subset of the original data we have at the node. If the data is pure, then our choice of what classification to make is rather simple. If the data is not pure, we will use the class that appears the most among the data points.\n\nclassify_data &lt;- function(data){\n  #Get labels\n  get_labels = pull(data[, -1])\n  #Get label frequency and max\n  label_freq = table(get_labels)\n  label_freq_a = as.data.frame(label_freq)\n  label_dom = max(label_freq)\n  #Get classification\n  for(i in 1:nrow(label_freq_a)){\n    if(label_freq_a$Freq[i] == label_dom){\n      classification = as.character(label_freq_a$get_labels[i])\n    } else {\n      next(i)\n    }\n  }\n  return(classification)\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#splitting-the-data",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#splitting-the-data",
    "title": "Decision tree classifier from scratch v1",
    "section": "Splitting the data",
    "text": "Splitting the data\nI am going to, un-intuitively, make the helper that will split the data before making the helper for the potential splits. Once we have a potential split value for a given feature we need to separate the parent node into two children. Anything above the value (\\(&gt;\\)) will be coined as the right node, and anything below the value (\\(\\leq\\)) will be termed the left node. These two nodes, together, are the children of the parent node.\n\nsplit_data &lt;- function(data, split_column, split_value){\n  split_c = data[[split_column]]\n  #Filter the data into above and below\n  data_below = data[split_c &lt;= split_value, ]\n  data_above = data[split_c &gt; split_value, ]\n  return(list(data_above, data_below))\n}"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#potential-and-best-splits",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#potential-and-best-splits",
    "title": "Decision tree classifier from scratch v1",
    "section": "Potential and best splits",
    "text": "Potential and best splits\nThe potential split helper(s) are arguably the most important helper. These helpers, as the name suggests, will search our data for the split that provides the most certainty regarding the classes of the child nodes. It will return a feature, and a value for that feature for which we must split. First and foremost, we need to consider the manner in which we will search our data. There are several ways to approach the search stage. One such way is to increment through the range of a feature by a learning rate; at each of these increment, we will calculate the entropy of a split made at that point. The effectiveness of this approach is largely determined by the learning rate. A very small learning rate will take a long time iterate through the data, but will be more accurate. The converse is also true for a large learning rate. Another approach would be to only have potential splits be made on each real value a feature has. The middle ground between these approaches is to check for a potential splits in the middle of two values for a given features. I will use the third approach as it is the easiest to implement\n\nget_potential_splits &lt;- function(data){\n  #Sorting stage\n  data = data\n  col_n = ncol(data) - 1\n  for(i in 1:col_n){\n    data_i = sort(data[, i])\n    data[, i] = data_i\n  }\n  #Creating the splits\n  dat = data[0, ]\n  for(j in 1:col_n){\n    for(i in 2:nrow(data)){\n      curr_val = data[i, j]\n      previous_val = data[(i-1), j]\n      potential_val = (curr_val + previous_val)/2\n      dat[(i-1), j] = potential_val\n    }\n  }\n  dat[nrow(dat)+1, ] = data[nrow(data), ]\n  dat = dat[, 1:col_n]\n  potential_splits = as.data.frame(dat)\n  if(ncol(potential_splits) == 1){\n    colnames(potential_splits)[[1]] = colnames(data)[[1]]\n  }\n  return(potential_splits)\n}\n\n\ncalculate_overall_entropy &lt;- function(data_below, data_above){\n  #Proportion of samples in left and right children\n  n = nrow(data_below) + nrow(data_above)\n  p_data_below = nrow(data_below)/n\n  p_data_above = nrow(data_above)/n\n  #Calculate overall entropy\n  overall_entropy = \n    ((p_data_below*get_entropy(as.character(pull(data_below[, -1])))) \n     + (p_data_above*get_entropy(as.character(pull(data_above[, -1])))))\n  return(overall_entropy)\n}\n\n\ndetermine_best_split &lt;- function(data, potential_splits){\n  #Initialize overall entropy and col \n  running_entropy = 9999\n  best_split_value = 0\n  best_split_column = \"\"\n  #Find best entropy over potential splits\n  for(j in 1:ncol(potential_splits)){\n    for(i in unique(potential_splits[, j])){\n      mask_val = i \n      mask_col = j\n      splits = \n        split_data(data = data, split_column = mask_col, split_value = mask_val)\n      relative_entropy = \n        calculate_overall_entropy(data_above = splits[[1]], \n                                  data_below = splits[[2]])\n      if(relative_entropy &lt; running_entropy){\n        running_entropy = relative_entropy\n        best_split_value = mask_val\n        best_split_column = colnames(potential_splits)[j]\n      } else {\n        next(i)\n      }\n    }\n  }\n  return(list(best_split_column, best_split_value))\n}\n\n\ndetermine_best_split &lt;- function(data, potential_splits, mode = \"gini\"){\n  #Initialize overall entropy and col \n  running_gain = -Inf\n  best_split_value = 0\n  best_split_column = \"\"\n  #Find best entropy over potential splits\n  for(j in 1:ncol(potential_splits)){\n    for(i in unique(potential_splits[, j])){\n      mask_val = i \n      mask_col = j\n      splits = \n        split_data(data = data, split_column = mask_col, split_value = mask_val)\n      relative_gain =\n        get_information_gain(parent = data, r_child = splits[[1]], \n                             l_child = splits[[2]], mode = mode)\n      if(relative_gain &gt; running_gain){\n        running_gain = relative_gain\n        best_split_value = mask_val\n        best_split_column = colnames(potential_splits)[j]\n      } else {\n        next(i)\n      }\n    }\n  }\n  return(list(best_split_column, best_split_value))\n}\n\n#might need to come back in future and add as.character() to gini as well"
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#recursive-function",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#recursive-function",
    "title": "Decision tree classifier from scratch v1",
    "section": "Recursive function",
    "text": "Recursive function\nThe main algorithm is a recursive function that calls the helpers to split the data, given that the stopping conditions have not been violated. In the below function, the stopping conditions are first checked. There are three conditions implemented. Namely, whether the data is fully pure, whether there is enough data points to create a new splitting node, and whether the maximum depth of the tree has been reached. The function then uses the helpers that were created earlier to recursively split the data, generating a ‚Äòyes‚Äô and ‚Äòno‚Äô answer. It prints all of this information as it does it; more nuanced code would likely build and print the tree in this same function, but I was unable to build the necessary code to do so\n\ndecision_tree_algorithm &lt;- function(df, \n                                    counter = 1, \n                                    min_samples, \n                                    max_depth, is.child = \"root\"){\n  data = df\n  #Check whether stopping conditions have been violated\n  if(any(check_purity(data), \n         nrow(data) &lt; min_samples, \n         (counter - 1) == max_depth)){\n    classification = classify_data(data)\n    return(print(paste(classification, is.child, counter)))\n  } else {\n   #Recursive part\n    \n    #Helper functions\n    potential_splits = get_potential_splits(data)\n    split_g = determine_best_split(data, potential_splits)\n    split_column = split_g[[1]]\n    split_value = split_g[[2]]\n    data_g = split_data(data, split_column, split_value)\n    data_above = data_g[[1]]\n    data_below = data_g[[2]]\n    print(paste(split_column, split_value, is.child, counter))\n    #Find the answers\n    yes_answer =\n      decision_tree_algorithm(df = data_below, \n                              counter = counter + 1, \n                              min_samples, max_depth, is.child = \"yes &lt;=\")\n    no_answer =\n      decision_tree_algorithm(df = data_above, \n                              counter = counter + 1, \n                              min_samples, max_depth, is.child = \"no &gt;\")\n    \n  }\n  \n}\n\nSo the main algorithm yields the following output on the salmon-fish data‚Ä¶\n\ndecision_tree_algorithm(df = salmon_fish, min_samples = 1, max_depth = 3)\n\n[1] \"length 2.99601479397271 root 1\"\n[1] \"tuna yes &lt;= 2\"\n[1] \"weight 4.00670743566802 no &gt; 2\"\n[1] \"salmon yes &lt;= 3\"\n[1] \"length 6.97822538046186 no &gt; 3\"\n[1] \"tuna yes &lt;= 4\"\n[1] \"tuna no &gt; 4\"\n\n\nNow this is obviously not what a decision tree looks like (as was shown earlier). But it is a step in the right direction. There are all the essential components of a decision tree in the jumble of output, but it requires sorting to be more comprehensible."
  },
  {
    "objectID": "posts/2024-12-04-decision-tree-classifier-v1/index.html#organising-function",
    "href": "posts/2024-12-04-decision-tree-classifier-v1/index.html#organising-function",
    "title": "Decision tree classifier from scratch v1",
    "section": "Organising function",
    "text": "Organising function\nThis function calls the decision tree recursive algorithm, captures the output and re-formats in into a data frame. An efficient implementation will likely not need this function, but since I was unable to fully incorporate the whole process in the recursive function, here we are.\n\ndecision_tree &lt;- function(df, min_samples, max_depth){\n  \n  #Store decisions and reformat\n  decisions =\n    capture.output(decision_tree_algorithm(df = df, \n                                           min_samples = min_samples, \n                                           max_depth = max_depth), append = F)\n  decisions_df = strsplit(decisions, \" \")\n  decisions_cl = sapply(decisions_df, function(x) gsub(\"\\\"\", \"\", x))\n  \n  #Make list of equal length\n  for(i in 1:length(decisions_cl)){\n  if(length(decisions_cl[[i]]) != 6){\n    if(i == 1){\n      decisions_cl[[i]] = append(decisions_cl[[i]], NA, after = 4)\n    } else {\n      decisions_cl[[i]] = append(decisions_cl[[i]], NA, after = 2)\n    }} else { next(i) }}\n  \n  #Convert to df and reformat again\n  decisions_df = as.data.frame(decisions_cl)\n  decisions_t = as.data.frame(t(decisions_df))\n  decisions_x = decisions_t[, -1]\n  row.names(decisions_x) = 1:nrow(decisions_x)\n  colnames(decisions_x) = c(\"node\", \"split.val\", \"is.child\", \"split\", \"depth\")\n  \n  #Sort the df\n  decisions_x = decisions_x[order(decisions_x$depth), ]\n  \n  return(decisions_x)\n}\n\nThe organising function returns output that looks like this\n\nsalmon_df = decision_tree(df = salmon_fish, min_samples = 1, max_depth = 2)\n\n\n\n\n\n\n\n\n\n\nThis is just what we got from the recursive function, but re-organised into a data frame format. Once again this is not in the tree format, but a rather a tabular view of each node in the tree. However if we compare our data frame on the right above to a decision tree created using rpart, it is clear that they are essentially the same. Ideally I would have wanted to somehow take the data frame that is returned from the organising function, and somehow printed it out in a tree format in the console. But that is a piece of the puzzle that I am yet to solve for now."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rashmin Chitale",
    "section": "",
    "text": "Hello and welcome to my personal website, your one stop shop for viewing my self-indulgent display of personal projects. I am a recent university graduate who studied actuarial studies. Some of my interests include mathematics, data science and cricket (the sportüèè). On this website, you will find some projects and other code related endeavors I have worked on in my downtime. Most of them are a from scratch implementation of a data science algorithm in R. Take a look if you are interested."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rashmin Chitale",
    "section": "Education",
    "text": "Education\nMacquarie University | Bachelor of Actuarial Studies with Bachelor of Applied Finance | 2021 - 2024\nRyde Secondary College | Higher School Certificate | 2015 - 2020"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Rashmin Chitale",
    "section": "Experience",
    "text": "Experience\nAustralian Bureau of Statistics | Analyst | January 2022 - Present"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html",
    "title": "K-nn from scratch",
    "section": "",
    "text": "The k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method. Non-parametric meaning the algorithm does not make any distributional assumptions about the data, and supervised meaning data with labels is used.\n\n‚ÄúThe k-nearest neighbors algorithm (k-NN) was first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors - Wikipedia‚Äù\n\nIn simple terms, the algorithm is passed a data set with labels, and a new point which is not in the provided data. The algorithm uses the proximity of other points from the data set near the new data point to make predictions about the individual point. This prediction can either be a classification, or a numeric value. Pictures always help, so lets take a look at one!\n\n\nIn the below image we have two features, namely \\(X_{1}\\) and \\(X_{2}\\). We also have two classes, A and B, which are respectively denoted by yellow and purple points. Our new point which is to be classified or regressed on is red. You might already be guessing how some of the terms such as ‚Äòproximity‚Äô and ‚Äòplurality vote‚Äô factor into this algorithm from the image, but I will spell it out for you JIC. So to assign the red point a class we look at its \\(k\\) nearest neighbors. The word neighbor implies nearness. So we look at the points that are closest to the red point. For \\(k=1\\), this is the single closes point, for \\(k=2\\) this is the two closest points, for \\(k=3\\) this is the three closest points and so on. Now, consider the \\(k=3\\) case show in the image. Of the three closest points to the red point, two are purple and one is yellow. If a ‚Äòplurality vote‚Äô was two occur, one yellow point would vote that the red point should be assigned to class A, whereas two purple points would vote that it should be class B. Clearly, the purple points win, so the red point would be assigned to class B. But this will not always be the case. With \\(k=6\\) it is direct that yellow points outnumber the purple, and hence the ‚Äòvote‚Äô is won by the yellow points; the red point will be assigned to class A.\n\n\n\n\n\n\n\n\n\n\n\n\nThere is always the chance that there will be an equal amount of the ‚Äòhighest vote number‚Äô for some \\(k\\). This means when the vote occurred, there was an equal amount of points belonging to two or more different classes (tied values). There are multiple ways to eliminate this tie.\n\nChoose a different \\(k\\): A tie will likely not exist for all potential values of \\(k\\). So we can change our \\(k\\). Simple enough. But arbitrarily choosing any other \\(k\\) does not ensure that there will be no ties\nRandomly choose between the tied values: Just as the name suggests\nAllow in until natural stop: This one is a little more nuanced than the others. Choose the smallest number \\(k\\) where \\(k \\geq 2\\) such that there exists no ties"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#visual-example",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#visual-example",
    "title": "K-nn from scratch",
    "section": "",
    "text": "In the below image we have two features, namely \\(X_{1}\\) and \\(X_{2}\\). We also have two classes, A and B, which are respectively denoted by yellow and purple points. Our new point which is to be classified or regressed on is red. You might already be guessing how some of the terms such as ‚Äòproximity‚Äô and ‚Äòplurality vote‚Äô factor into this algorithm from the image, but I will spell it out for you JIC. So to assign the red point a class we look at its \\(k\\) nearest neighbors. The word neighbor implies nearness. So we look at the points that are closest to the red point. For \\(k=1\\), this is the single closes point, for \\(k=2\\) this is the two closest points, for \\(k=3\\) this is the three closest points and so on. Now, consider the \\(k=3\\) case show in the image. Of the three closest points to the red point, two are purple and one is yellow. If a ‚Äòplurality vote‚Äô was two occur, one yellow point would vote that the red point should be assigned to class A, whereas two purple points would vote that it should be class B. Clearly, the purple points win, so the red point would be assigned to class B. But this will not always be the case. With \\(k=6\\) it is direct that yellow points outnumber the purple, and hence the ‚Äòvote‚Äô is won by the yellow points; the red point will be assigned to class A."
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#an-interesting-case",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#an-interesting-case",
    "title": "K-nn from scratch",
    "section": "",
    "text": "There is always the chance that there will be an equal amount of the ‚Äòhighest vote number‚Äô for some \\(k\\). This means when the vote occurred, there was an equal amount of points belonging to two or more different classes (tied values). There are multiple ways to eliminate this tie.\n\nChoose a different \\(k\\): A tie will likely not exist for all potential values of \\(k\\). So we can change our \\(k\\). Simple enough. But arbitrarily choosing any other \\(k\\) does not ensure that there will be no ties\nRandomly choose between the tied values: Just as the name suggests\nAllow in until natural stop: This one is a little more nuanced than the others. Choose the smallest number \\(k\\) where \\(k \\geq 2\\) such that there exists no ties"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#euclidean-distance",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#euclidean-distance",
    "title": "K-nn from scratch",
    "section": "Euclidean distance",
    "text": "Euclidean distance\nEuclidean distance is a perhaps the most common and well known measure of distance in mathematics. As I recall from my golden high school days, it is used just about everywhere; complex numbers, vectors, trigonometry, calculus and so on.\n\n‚ÄúIn mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance.‚Äù\n\n\n\n\n\n\n\n\n\n\nIn two-dimensions the Euclidean distance is simple enough. It follows directly from Pythagoras theorem, and is nothing more than the sum of the squared differences of the \\(x\\) and \\(y\\) coordinates. Suppose we have two points in the Cartesian space \\(p = (p_{1}, p_{2})\\) and \\(q = (q_{1}, q_{2})\\). Then the Euclidean distance is\n\\[\\begin{align*}\n\nd(p,q) = \\sqrt{(q_{1}-p_{1})^2 +(q_{2}-p_{2})^2}\n\n\\end{align*}\\]\nSome fancy geometric proofs, and pattern observing lands us the \\(n\\) dimensional Euclidean distance, which is not too different from the two dimensional one. Suppose we have two points in a \\(n\\) dimensional space, \\(p = (p_{1}, p_{2}, ... , p_{n})\\) and \\(q = (q_{1}, q_{2}, ... , q_{n})\\) . Then the Euclidean distance is \\[\\begin{align*}\n\nd(p,q) = \\sqrt{(q_{1}-p_{1})^2 + (q_{2}-p_{2})^2 + (q_{3}-p_{3})^2 + ... + (q_{n}-p_{n})^2}\n\n\\end{align*}\\]\nThis is the metric I will use to establish ‚Äòproximity‚Äô later due to its simplicity, and appropriateness for the latter notion. But it is important to note that the Euclidean distance does not perform as well in very high dimensions numbers. Enter, the curse of dimensionality!.\n\n‚ÄúThe curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector‚Äù\n\nJust to spice things up, I will throw another distance metric into the bag, but lets first encode the euclidean distance metric\n\nget_euclid &lt;- function(p, q){\n  #Check for same length\n  if(length(p) != length(q)) return(\"Error, unequal length!\")\n  #Calculate distance\n  distance = sqrt(sum((q-p)^2))\n  return(distance)\n}"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#cosine-distance",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#cosine-distance",
    "title": "K-nn from scratch",
    "section": "Cosine distance",
    "text": "Cosine distance\n\nCosine similarity/distance measures the similarity between two vectors of an inner product space. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. Using this distance we get values between 0 and 1, where 0 means the vectors are 100% similar to each other and 1 means they are not similar at all\n\nCosine distance has the below formula\n\\[\\begin{align*}\n\n\\cos{\\theta} = \\frac{\\overrightarrow{a}\\cdot\\overrightarrow{b}}{||\\overrightarrow{a}|| \\times ||\\overrightarrow{b}||}\n\n\\end{align*}\\]\nThose of you familiar with vectors and linear algebra will note that cosine similarity is just a re-arrangement of the dot product formula. The closer the cosine value is to 1, the smaller the angle between the two vectors, and the greater the match between the two vectors. And vice versa. Earlier I mentioned that euclidean distance breaks down in higher dimensions. Since cosine distance looks how closely two points are oriented to each other, it deals with data with a large number of dimensions better (i.e.¬†when the data is sparse)\n\nget_cosine &lt;- function(p, q){\n  #Check for same length\n  if(length(p) != length(q)) return(\"Error, unequal length!\")\n  #Calculate cosine distance\n  cos_dot = sum(p*q)\n  p_mag = sqrt(sum(p^2))\n  q_mag = sqrt(sum(q^2))\n  cos_dis = 1 - cos_dot/(p_mag*q_mag)\n  return(round(cos_dis, digits = 10))\n}"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#classification",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#classification",
    "title": "K-nn from scratch",
    "section": "Classification",
    "text": "Classification\nThe kNN algorithm for classification calculates the distance between the new point and all the other points in the data set. It sorts the distances from the shortest to furthest and then chooses the \\(k\\) smallest distances. The get_majority_vote() helper is called on the labels of theses \\(k\\) smallest distances to produce the dominant label which is then returned by the function. As it turns out the regression problem is even easier than that of classification. A majority vote helper is not even required. We simply find the \\(k\\) nearest neighbors, and average the value of the target variable to find the value for the new point.\n\nknn_classify &lt;- function(df, k, new_point, type){\n  #Check the new point is of same length\n  if(length(new_point) != ncol(df) - 1) return(\"Error, unequal length!\")\n  #Calculate the distances and order them\n  df_euclid = df[, (ncol(df) - 1):ncol(df)] \n  df_points = as.data.frame(df[, 1:(ncol(df)-1)])\n  for(i in 1:nrow(df_euclid)){\n    df_euclid[i, 1] = get_euclid(p = new_point, \n                                 q = as.numeric(df_points[i, ]))\n  }\n  df_order = df_euclid[order(df_euclid[,1]), ]\n  #Return the k-closest neighbors\n  df_knn = df_order[1:k, ]\n  labels_k = df_knn[, 2]\n  #The dominating label\n  dom_label = get_majority_vote(labels = labels_k)\n  return(dom_label)\n}"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#regression",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#regression",
    "title": "K-nn from scratch",
    "section": "Regression",
    "text": "Regression\nAs it turns out the regression problem is even easier than that of classification. A majority vote helper is not even required. We simply find the \\(k\\) nearest neighbors, and average the value of the target variable to find the value for the new point.\n\nknn_regress &lt;- function(df, k, new_point){\n  #Check the new point is of same length\n  if(length(new_point) != ncol(df) - 1) return(\"Error, unequal length!\")\n  #Calculate the distances and order them\n  df_euclid = df[, (ncol(df) - 1):ncol(df)] \n  df_points = as.data.frame(df[, 1:(ncol(df)-1)])\n  for(i in 1:nrow(df_euclid)){\n    df_euclid[i, 1] = get_euclid(p = new_point, \n                                 q = as.numeric(df_points[i, ]))\n  }\n  df_order = df_euclid[order(df_euclid[,1]), ]\n  #Return the k-closest neighbors\n  df_knn = df_order[1:k, ]\n  #The average value\n  regress_val = mean(df_knn[, 2])\n  return(regress_val)\n}\n\nknn_regress(df = iris[,1:3], k = 1, new_point = c(7, 4))\n\n[1] 6.1"
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#classification-performance",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#classification-performance",
    "title": "K-nn from scratch",
    "section": "Classification performance",
    "text": "Classification performance\n\nModel validation\nOne way we can evaluate the classifier is by splitting the iris data into a training and testing set. The training set will consist of points that will be used the ‚Äúnew points‚Äù in the testing set. We can then compare the classifications outputted by using the algorithm on the testing set versus what the classifications actually are.\n\n#Train test split\nset.seed(21312)\ndt = sort(sample(nrow(iris), nrow(iris)*.7))\ntrainData &lt;- iris[dt,]\ntestData &lt;- iris[-dt,]\n\n#Accuracy of k for dif vals on one possible split\naccuracy = c()\nfor(j in 1:20){\nfor(i in 1:nrow(testData)){\ntestData[i, 6] = \n  knn_classify(df = trainData, k = j, \n               new_point = c(as.numeric(testData[i, 1:4])))\nif(testData[i, 6] == testData[i, 5]){\n  testData[i, 7] = TRUE\n} else {\n  testData[i, 7] = FALSE\n}}\naccuracy[j] = length(which(testData[, 7] == TRUE))/nrow(testData)\n}\naccuracy\n\n [1] 0.9333333 0.9333333 0.9555556 0.9555556 1.0000000 1.0000000 1.0000000\n [8] 1.0000000 0.9777778 0.9777778 0.9333333 0.9333333 0.9555556 0.9555556\n[15] 0.9333333 0.9333333 0.9555556 0.9555556 0.9333333 0.9333333\n\n#Confusion matrix for k = 5\nfor(i in 1:nrow(testData)){\ntestData[i, 6] = \n  knn_classify(df = trainData, k = 20, \n               new_point = c(as.numeric(testData[i, 1:4])))\nif(testData[i, 6] == testData[i, 5]){\n  testData[i, 7] = TRUE\n} else {\n  testData[i, 7] = FALSE\n}}\ntable(testData$Species, testData$V6) \n\n            \n             setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         14         1\n  virginica       0          2        15\n\n\nFor \\(k=20\\) the confusion matrix (mis-classifcation matrix), shows that the model performs quite well. Two virginica species were incorrectly classified as versicolor. One versicolor species was incorrectly classified as virginica. Else all seems well. It is also clear that for many different values of \\(k\\), the model has 100% accuracy. This means that it can correctly classify samples from the training data set by considering the proximity to points in the train data set. A 100% accuracy is not always good. It may indicate that the k-NN model is over fitted to the data, and will not generalise to new data points. This is closely related to the notion of the bias-variance tradeoff. I will not delve into the specifics for now, as this concept deserves its own explanation.\nBack to the problem at hand. Earlier we saw that there were many values of \\(k\\) which produced an accuracy of 100% on one particular train test split. However if we shuffled the data, or changed the train test split ratio, the model would perform differently for the same value of \\(k\\). Surely, there is a way to find out which \\(k\\) performs the best on average across different train test splits for our data? Enter k-fold cross validation.\n\n#Perform 10 fold cross validation\nk = 10\nz = 40\nset.seed(23131)\nk_fold_data = \n  mutate(iris, my.folds = sample(1:k, size = nrow(iris), replace = TRUE))\ntable(k_fold_data$my.folds)\n\n\n 1  2  3  4  5  6  7  8  9 10 \n15 19 17 17 12 17 14 14 12 13 \n\n#a = z is no of neighbors, b is which partition to use as test\nk_fold_accuracy = vector(\"list\", z)\nfor(a in 1:z){\n  for(b in 1:k){\n    #Split into train and test based off which partition to use\n    k_fold_test = k_fold_data[k_fold_data$my.folds == b, ]\n    k_fold_train = k_fold_data[k_fold_data$my.folds != b, 1:5]\n    \n    #For given a,b check if predicted matches true label\n    for(i in 1:nrow(k_fold_test)){\n      k_fold_test[i, 7] = \n        knn_classify(df = k_fold_train, \n                     k = a, \n                     new_point = c(as.numeric(k_fold_test[i, 1:4])))\n     if(k_fold_test[i, 5] == k_fold_test[i, 7]){\n       k_fold_test[i, 8] = TRUE\n     } else {\n        k_fold_test[i, 8] = FALSE\n      }\n    }\n    \n    #Fill list with accuracy\n    k_fold_accuracy[[a]][b] =\n      length(which(k_fold_test[, 8] == TRUE))/nrow(k_fold_test)\n    next(b)\n  }\n  next(a)\n}\n\n#Weights of each partition, then multiply to get average\nk_fold_weights = as.numeric(table(k_fold_data$my.folds)/nrow(k_fold_data))\nfor(i in 1:length(k_fold_accuracy)){\n  k_fold_accuracy[[i]] = sum(k_fold_accuracy[[i]]*k_fold_weights)\n}\n\nas.numeric(k_fold_accuracy)\n\n [1] 0.9600000 0.9600000 0.9666667 0.9666667 0.9600000 0.9600000 0.9666667\n [8] 0.9666667 0.9600000 0.9600000 0.9666667 0.9666667 0.9800000 0.9800000\n[15] 0.9800000 0.9800000 0.9800000 0.9800000 0.9666667 0.9666667 0.9800000\n[22] 0.9800000 0.9733333 0.9733333 0.9733333 0.9733333 0.9600000 0.9600000\n[29] 0.9533333 0.9533333 0.9533333 0.9533333 0.9600000 0.9600000 0.9666667\n[36] 0.9666667 0.9600000 0.9600000 0.9533333 0.9533333\n\n\nAverage model performance never reaches 100% accuracy for any \\(k\\). As can seen above the highest average accuracy attained by the model is 98%, which occurs for several different \\(k\\) values. Overall, if we said that all \\(k\\)‚Äôs of a reasonably small magnitude perform similarly, we would not be wrong.\n\n#Format data for error\nk_fold_plot = as.data.frame(t(as.data.frame(k_fold_accuracy)))\nrow.names(k_fold_plot) = 1:nrow(k_fold_plot)\nk_fold_plot$k = 1:nrow(k_fold_plot)\ncolnames(k_fold_plot) = c(\"Error (%)\", \"k\")\nk_fold_plot$Error = (1 - k_fold_plot$Error)*100 \n\n#Create line plot\nggplot(data = k_fold_plot, aes(x = k, y = Error)) +\n  geom_line(color = \"orange\", size = 1.5) +\n    theme_classic() +\n  labs(title = \"Cross validated error rate by differing k's\", \n       subtitle = \"Which k performs the best on average?\") +\n    theme(text=element_text(family=\"montsr\"), \n        plot.title = element_text(size=32),\n        plot.subtitle = element_text(size=22),\n        axis.title = element_text(size=22),\n        axis.text = element_text(size=16),\n        legend.text = element_text(size=16), \n        legend.title = element_text(size=22)) +\n  scale_color_pilot()\n\n\n\n\n\n\n\n\n\n\nReason for good performance\nPerhaps, the reason why \\(k\\) does not appear to significantly affect model performance lies in the structure of the iris data set.\n\n#Pair-wise plot\nggpairs(data = iris, columns = c(\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"),\n  lower = list(continuous = \"points\", mapping = aes(color = Species)), \n  upper = list(continuous = \"points\", mapping = aes(color = Species)), \n  diag = list(continuous = \"barDiag\", mapping = aes(color = Species)))\n\n\n\n\n\n\n\n\nOn every pairwise combination of the data, there appears to be significant grouping of the different classes. This shows that the iris data is well suited to the k-NN algorithm. Given that the grouping is quite significant, this confirms the relative indifference of the accuracy of the algorithm to different values of \\(k\\). However, the above plot is only a visualization of pairwise combinations in two dimensions. It would be nice if there were a way to visualize the grouping of classes for all four features of the data.\n\n#Format data for plot\niris_id = iris\niris_id[, 6] = 1:nrow(iris)\niris_parallel_plot = iris_id %&gt;%  \n  pivot_longer(cols = c(\"Sepal.Length\", \"Sepal.Width\", \n                        \"Petal.Length\", \"Petal.Width\"), \n              names_to = \"Feature\", values_to = \"Feature.Value\")\n\n#Make plot\nggplot(data = iris_parallel_plot, \n       aes(x = Feature, y = Feature.Value, color = Species, group = V6)) +\n  geom_point() + \n  geom_line() + \n  scale_x_discrete(limits = c(\"Sepal.Length\", \n                              \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")) +\n  theme_classic() +\n  labs(title = \"Parallel lines plot of iris\", \n       subtitle = \"Is iris well suited for the k-NN classification\n       algorithm?\") +\n    theme(text=element_text(family=\"montsr\"), \n        plot.title = element_text(size=32),\n        plot.subtitle = element_text(size=22),\n        axis.title = element_text(size=22),\n        axis.text = element_text(size=16),\n        legend.text = element_text(size=16), \n        legend.title = element_text(size=22)) +\n  scale_color_pilot()\n\n\n\n\n\n\n\n\nThe above is a visualization of the multivariate data, known as a parallel lines plot. Since we are searching for a reason for good model performance with the k-NN algorithm, we are looking for grouping in the plot. What I mean by grouping, is that different observations (rows) on iris for a given species follow a similar trajectory across all features; i.e.¬†they are nearby to each other. Given grouping holds relatively well, one thing in addition to consider is distinctness of the trajectories for each species. In the above plot, the setosa species has distinct shape. Versicolor and virginica are similar. This could also be seen earlier in the pair plots.\n\nIn data visualization, an Andrews plot or Andrews curve is a way to visualize structure in high-dimensional data. It is basically a rolled-down, non-integer version of the Kent‚ÄìKiviat radar m chart, or a smoothed version of a parallel coordinate plot. It is named after the statistician David F. Andrews.A value \\(x\\) is a high-dimensional datapoint if it is an element of \\(\\mathbb{R}^{d}\\). We can represent high-dimensional data with a number for each of their dimensions, \\(x=\\{x_{1},x_{2},\\ldots ,x_{d}\\}\\). To visualize them, the Andrews plot defines a finite Fourier series:\\(f_{x}(t)={\\frac {x_{1}}{\\sqrt{2}}}+x_{2}\\sin(t)+x_{3}\\cos(t)+x_{4}\\sin(2t)+x_{5}\\cos(2t)+\\cdots\\)\n\n\n\n\n\n\n\n\n\n\nThe Andrews Curves for iris are the smoothed out versions of the parallel lines plot. For iris, they confirm what we saw earlier regarding the grouping of classes."
  },
  {
    "objectID": "posts/2024-11-26-k-nearest-neighbors/index.html#regression-performance",
    "href": "posts/2024-11-26-k-nearest-neighbors/index.html#regression-performance",
    "title": "K-nn from scratch",
    "section": "Regression performance",
    "text": "Regression performance\nOur discussion of the performance of k-NN in regression for different values of \\(k\\), will center around the calculation of the cross validated mean squared error (MSE).\n\nThe mean squared error (MSE) of an estiamtor measures the average square of the errors; the average squared difference between the stimated values and the actual value. It is strictly positive, and represents the quality of an estimator. MSE decreases as the error approaches zero\n\nMSE has the following formula; \\(n\\) is the number of observations, \\(Y_{i}\\) is the \\(i\\)th observed value of the variable, \\(\\hat{Y}_{i}\\) is the \\(i\\)th predicted value of the variable\n\\[\\begin{align*}\n\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n}{(Y_{i} - \\hat{Y_{i}})^2}\n\n\\end{align*}\\]\nIn addition to this, I will also calculate mean absolute percentage error (MAPE) and mean absolute error (MAE). MAE gives how far on average, each predicted value is from the true value in distance. MAPE is the same as MAE but a percentage\n\\[\\begin{align*}\n\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n}{|Y_{i} - \\hat{Y_{i}}|}\n\n\\end{align*}\\] \\[\\begin{align*}\n\n\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n}{\\frac{|Y_{i} - \\hat{Y_{i}}|}{Y_{i}}}\n\n\\end{align*}\\]\nTo calculate regression performance, I will use the first three variables of the iris data to estimate the fourth variable\n\n#Perform 10 fold cross validation\nk = 10\nz = 40\nset.seed(23321)\nk_fold_data = mutate(iris, my.folds = sample(1:k, size = nrow(iris), \n                                             replace = TRUE))\ntable(k_fold_data$my.folds)\n\n\n 1  2  3  4  5  6  7  8  9 10 \n17 12 16 14 10 16 14 15 21 15 \n\n#a = z is no of neighbors, b is which partition to use as test\nk_fold_accuracy = vector(\"list\", z)\nk_fold_accuracy1 = vector(\"list\", z)\nk_fold_accuracy2 = vector(\"list\", z)\nfor(a in 1:z){\n  for(b in 1:k){\n    #Split into train and test based off which partition to use\n    k_fold_test = k_fold_data[k_fold_data$my.folds == b, 1:4]\n    k_fold_train = k_fold_data[k_fold_data$my.folds != b, 1:4]\n    \n    #For given a,b check if predicted matches true label\n    for(i in 1:nrow(k_fold_test)){\n      k_fold_test[i, 5] = \n        knn_regress(df = k_fold_train,\n                      k = a, \n                        new_point = as.numeric(k_fold_test[i, 1:3]))\n      k_fold_test[i, 6] = \n        (k_fold_test[i, 4] - k_fold_test[i, 5])^2\n      k_fold_test[i, 7] = \n        abs(k_fold_test[i, 4] - k_fold_test[i, 5])\n      k_fold_test[i, 8] = \n        abs(k_fold_test[i, 4] - k_fold_test[i, 5])/k_fold_test[i, 4]\n    }\n    #Fill list with accuracy\n    k_fold_accuracy[[a]][b] = (1/nrow(k_fold_test))*sum(k_fold_test[, 6])\n    k_fold_accuracy1[[a]][b] = (1/nrow(k_fold_test))*sum(k_fold_test[, 7])\n    k_fold_accuracy2[[a]][b] = (1/nrow(k_fold_test))*sum(k_fold_test[, 8])\n    next(b)\n  }\n  next(a)\n}\n\n#Weights of each partition, then multiply to get average\nk_fold_weights = as.numeric(table(k_fold_data$my.folds)/nrow(k_fold_data))\nfor(i in 1:length(k_fold_accuracy)){\n  k_fold_accuracy[[i]] = sum(k_fold_accuracy[[i]]*k_fold_weights)\n  k_fold_accuracy1[[i]] = sum(k_fold_accuracy1[[i]]*k_fold_weights)\n  k_fold_accuracy2[[i]] = sum(k_fold_accuracy2[[i]]*k_fold_weights)\n}\n\n#Making three line plots\nk_fold_mse = as.data.frame(t(as.data.frame(k_fold_accuracy)))\nrow.names(k_fold_mse) = 1:nrow(k_fold_mse)\ncolnames(k_fold_mse) = c(\"MSE\")\nk_fold_mse$k = 1:nrow(k_fold_mse)\n\nk_fold_mae = as.data.frame(t(as.data.frame(k_fold_accuracy1)))\nrow.names(k_fold_mae) = 1:nrow(k_fold_mae)\ncolnames(k_fold_mae) = c(\"MAE\")\nk_fold_mae$k = 1:nrow(k_fold_mae)\n\nk_fold_mape = as.data.frame(t(as.data.frame(k_fold_accuracy2)))\nrow.names(k_fold_mape) = 1:nrow(k_fold_mape)\ncolnames(k_fold_mape) = c(\"MAPE\")\nk_fold_mape$k = 1:nrow(k_fold_mape)\nk_fold_mape$MAPE = k_fold_mape$MAPE * 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn terms of MSE, it seems that at \\(k=6\\), MSE is the closes to zero. However, in terms of the MAE and MAPE, \\(k=10\\) seems to be the best. This is as for \\(k=10\\) the k-NN regressor is on average 0.14 units away from the true value, or around 19% away. One thing is clear though. The k-NN algorithm performs much better in the classification problem, then regression on the iris data set."
  },
  {
    "objectID": "posts/2024-12-11-k-fold-cv/index.html",
    "href": "posts/2024-12-11-k-fold-cv/index.html",
    "title": "K-fold cross validation from scratch",
    "section": "",
    "text": "A key term in k-fold cross validation is the last one. As such, I would like to preface my implementation of the algorithm, with a discussion of what model validation itself is. The word validation does not have unambiguous meaning. It is the affirmation or recognition of the validity of something. On this basis, it is pretty easy to take a guess at what model validation is. Rather than slapping the definition in right here, lets first take a step back and remind ourselves of what machine learning is, as this will assist us.\n\nMachine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. The models that are constructed by machine learning algorithms are then used to provide insight.\n\nSo machine learning is all about making predictions and models learning from known data such that they are able to generalise their learning to new data. And these predictions come after various processes including data preparation, model selection, model training and parameter tuning; the umbrella term used for these processes is model development. Model validation then occurs, followed by model implementation\n\nModel validation is the set of processes and activities intended to verify that models are performing as expected\n\nIn this sense, model validation is all about checking whether the model achieve its intended purpose. Just at the name suggests, the model seeks validation. While we now know that k-fold CV is attempting to check whether the model ‚Äúperforms as expected‚Äù, our understanding of the algorithm has not gotten particularly deep. To remedy that, I will once again digress.\nThe realm of machine learning is a wide one indeed. There are many models, utilizing different approaches to attempt to imitate human learning. Broadly speaking, approaches can be divided into three groups\n\nSupervised learning: This approach is characterized by the practice of building mathematical models on a set of data that contain both inputs and the desired output. The data is commonly labelled as being training data. Each data point or row in the training data set contains features (a.k.a covariates) and an associated label. This training data set is also commonly known as the feature vector; it can be decomposed into the covariates and the supervisory signal. An algorithm is then applied to the training data, which produces an inferred function which maps each data point (covariates or predictors) to some label or value. The ultimate goal is for the algorithm to build a function or model which learns from the training data set, and is then able to classify or regress on unseen data points which it was not trained on. Some algorithms will produce a function which is already best fitted to the training data, for example logistic regression, whereas others, may require hyperparameter tuning, an example being, k-NN.\nUnsupervised learning: In this approach we provide a data set to the algorithm, but omit the labels. An algorithm of this approach then finds structure in the data (grouping and clustering). These algorithms do not respond from ‚Äòfeedback‚Äô as they do not use training data that is labeled or classified.\nReinforcement learning: ‚ÄúAn area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward‚Äù. Straight from Wikipedia."
  },
  {
    "objectID": "posts/2024-12-11-k-fold-cv/index.html#train-test-split",
    "href": "posts/2024-12-11-k-fold-cv/index.html#train-test-split",
    "title": "K-fold cross validation from scratch",
    "section": "Train-test split",
    "text": "Train-test split\n\n\n\n\n\n\n\n\n\nPhew, its good that we got that out the way. What the above chunky block of words served to illustrate is that much of machine learning is about a model learning on some data, and then predicting on new examples. Now back to the question of how can we validate/benchmark/test a model? In any context, we would definitely want to test the ‚Äúgeneralization performance‚Äù of our model before applying it. Well if a model is built using data, then it makes sense that the only way to test it is by using data. But where does that testing data come from? This is where the train-test split comes in. Rather than using all of our available data to train our model, we split the data into a training partition, upon which the algorithm learns, and a testing partition which is not trained upon and serves as that ‚Äúnew/unseen‚Äù data. These splits can be of any form \\(X-Y\\) where \\(X\\) is the percentage of the data reserved to training the model, and \\(Y\\) is the rest used for testing.\n\ntrain_test_split &lt;- function(data, test){\n  #Store data\n  t_data = data\n  #Assign amount of samples\n  if(test &lt; 1){\n    sample_test = ceiling(test*nrow(t_data))\n    sample_train = nrow(t_data) - sample_test\n  } else {\n    sample_test = test\n    sample_train = nrow(t_data) - sample_test\n  }\n  #Shuffle data \n  t_data = t_data[sample(nrow(t_data)), ]\n  row.names(t_data) = 1:nrow(t_data)\n  #Assign partition and indices\n  indices = \n    sample(x = sample(1:nrow(t_data), size = nrow(t_data), replace = FALSE))\n  primary = rep(x = c(\"train\"), sample_train)\n  secondary = rep(x = c(\"test\"), sample_test)\n  total = append(primary, secondary)\n  t_data$my.folds[indices] = total\n  return(t_data)\n}\n\nJust for the sake of example, I have shown how you can use the code in conjunction with other packages, to calculate a performance metric.\n\n#Example of using a train test split\nset.seed(23891)\nttsplit = train_test_split(data = iris, test = 0.1)\nNBclassifier = \n  naiveBayes(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, \n                          data  = subset(ttsplit, ttsplit$my.folds == \"train\"))\nttsplit1 = subset(ttsplit, ttsplit$my.folds == \"test\")[, -6]\nttsplit1$prediction = predict(NBclassifier, newdata = ttsplit1, type = \"class\")\ntable(ttsplit1$Species, ttsplit1$prediction)\n\n            \n             setosa versicolor virginica\n  setosa          3          0         0\n  versicolor      0          5         1\n  virginica       0          0         6\n\n\nAs can be seen from the confusion matrix, one versicolor was classified as virginica from the naive Bayes classifier."
  },
  {
    "objectID": "posts/2024-12-11-k-fold-cv/index.html#k-fold-cross-validation",
    "href": "posts/2024-12-11-k-fold-cv/index.html#k-fold-cross-validation",
    "title": "K-fold cross validation from scratch",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\n\n\n\n\n\n\n\n\nK-fold cross validation expands upon the concept of the train-test split. In k-fold cross validation, we first randomly split our data to \\(k\\) partitions. We then use \\(k-1\\) of those partitions to create the training set, with the remaining partition becoming the testing set. This is repeated until each partition has served as the testing set. For example, 10 fold cross validation would split the data into 10 folds; 10% of the data would be used for testing, and 90% would be used for training. Note that, as you increase \\(k\\) the size of the testing partition becomes smaller, while the training partition becomes bigger. I will discuss an extreme case of this, where \\(k\\) is the largest possible size it can be later.\nYou may be wondering why k-fold cross validation is much better than the single train-test split. There are numerous reasons.\n\nUse of all data: A simple reason for sure. In a single test-split, some samples will be used for training, while others are reserved for testing. In k-fold, all of the available will take turns in being used for training and testing. In this regards, k-fold certainly leverages whatever data one may have to a better degree\nReduced variance of the performance metric: A notable issue with the single train-test split is that there is a very large number of potential train-test splits. Each of these splits will produce a different value of the performance estimate when the trained model is tested against the testing set. K-fold cross validation will let us average the performance estimate \\(k\\) times across unique train-test splits. This is especially useful as it is possible that one of those splits could be biased, or not represent the data particularly well. Model performance could be too high or too low in that case, but the algorithm would average this out. Overall, we come out, with a more robust estimate of the chosen models performance; and especially its ability to generalize to unseen data.\nGood for limited data: If data is sparse, a single train-test split will lead to a high variance in the performance metric due to skewed split. Resampling this small data set multiple times with k-fold cross validation will alleviate this issue.\nLarge data sets: With large data sets (think millions, if not billions of rows) training the model and testing it might be too computationally expensive even for a single train-test split. It might be better to take a subset of the whole data, say 100,000 of those rows, and perform k-fold cross validation.\nHyper-parameter tuning: Many machine learning algorithms come with a slight flexibility in their design. They allow the user to a change a value, which changes how the algorithm operates. K-fold cross validation allows the user to check which value for a certain model has the best performance.\nModel comparison: If our estimate of the performance metric is now more robust via k-fold cross validation ‚Äî while a single train-test split is a snapshot ‚Äî then we are now better able compare the performance of different machine learning models\n\nK-fold cross validation is not without its disadvantages\n\nMore computationally expensive: K-fold cross validation require way more operations to perform, given the additional steps it performs in comparison to a single split. This also makes it longer to perform, as the model needs to be trained \\(k\\) times, and evaluate \\(k\\) times as well."
  }
]