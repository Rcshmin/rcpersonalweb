{
  "hash": "95bfde814306fff9aaa71318c34b4555",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Decision tree classifier from scratch v2\"\ndescription: \"Fixing my previous attempt using the OOP framework\"\nauthor:\n  - name: Rashmin Chitale\n    url: \ndate: 01-19-2023\ncategories: [datascience,classicalml,R] # self-defined categories\nimage: images/dtree-oop-class-listing.png\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n\n\n\n# Preface\n\nThis was my second attempt at creating a decision tree classifier. When I last attempted the problem of creating a decision tree classifier from scratch, my final solution was very incomplete. I was able to create a so called \"recursive main function\", that would print all the nodes and store them in a data frame. However, I was unable to convert these splits into a tree like format, paralleling that of those found in the `rpart` and `tree` libraries. My solution was also unable to make a prediction on a new data point, rather, it stopped at learning on the tree. So in essence, what I had made, was not even complete in the slightest sense. A model that cannot be tested on new unseen data, is essentially an useless model in the realm of machine learning. \n\nThere were a few key reasons why I was unable to get to a good working solution. Firstly, there were a lot of variables and information that had to be kept tracked of. As the main recursive function generates the nodes, it was hard to store them in a tidy way, that could later be used to print the tree, and even have another function descend down it to get a new prediction. This stemmed in part from my poor understanding of how recursion worked on binary trees at the time. More so, my functional programming skills had hit a roadblock for this problem. I was also unsure whether the main recursive splitting function was working as it should, largely due to the inability to view the output of the function in a meaningful \"tree-like\" way. Looking back, all of these problems stemmed from one global issue â€” not having an adequate data structure. I was trying to create a decision tree using a data frame representation, when in actuality the best way to represent a tree, is just a tree. \n\n**Object Oriented Programming (OOP)** turned out to be the best solution to this problem. \n\n> Object-oriented programming (OOP) is a programming paradigm based on the concept of \"objects\", which can contain data and code. The data is in the form of fields (often known as attributes or properties), and the code is in the form of procedures (often known as methods). A common feature of objects is that procedures (or methods) are attached to them and can access and modify the object's data fields. In this brand of OOP, there is usually a special name such as this or self used to refer to the current object. In OOP, computer programs are designed by making them out of objects that interact with one another\n\nIn laymans terms OOP differs from procedural programming, in that it places more emphasis on the structure of the data \n\n> Most OOP lanuages define a class. A class is an abstract blueprint that creates more specific, concrete objects. Classes often represent broad categories, like Car or Dog that share attributes. These classes define what attributes an instance of this type will have, like color, but not the value of those attributes for a specific object. Classes can also contain functions called methods that are available only to objects of that type. These functions are defined within the class and perform some action helpful to that specific object type\n\nOnce again what this means is that we can create a blueprint for a data type, and then call specific functions on that data. These function can access and modify the attributes of this new data type (most of the time!). I have shown a simple example of this below for \"rectangles\". Rectangles is a not a data type that is default implemented in most languages like vectors or lists. But it is easy to define a rectangle geometrically, so let us make a special data type for rectangles (a class). We will then create a special function to act on this new data type; it will get the area.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRectangles = \n  setRefClass(Class = \"rectangles\",\n              fields = list(x = \"numeric\", y = \"numeric\"), \n              methods = \n                list(get_area = \n                       function(){\n                             return(.self$x*.self$y)\n                           },\n                     get_squared_area = \n                       function(){\n                             r_area = .self$get_area()\n                             return(.self$r_area^2)\n                           },\n                     get_cubed_area = \n                       function(){\n                             r_area = .self$get_area()\n                             return(r_area^3)\n                           }\n                         ))\n\nr1 = Rectangles(x = 5, y = 4)\nr1$get_area()\n## [1] 20\nr1$get_cubed_area()\n## [1] 8000\n```\n:::\n\n\n\nSo OOP is all about grouping related functions and variables together. Rather than having the functions and variables be made in isolation, which is what procedural programming focuses on, lets make them together. This was the technique which I needed to finish the problem of creating a decision tree classifier from scratch. Lastly there are a few more things I want to note about OOP. Programmers and computer science courses will often reference the \"four pillars\" behind OOP. Here is what they are. \n\n* **Inheritance**: Child classes inherit data and behaviors from the parent class\n* **Encapsulation**: Containing information in an object, exposing only selected information\n* **Abstraction**: Only exposing high-level public methods for accessing an object\n* **Polymorphism**: Many methods can do the same task\n\nR has several different ways of doing OOP, each using the four pillars to a different degree. These OOP systems are called S3, S4 and R6 (extension to RC). There are different trade off between using different systems. For those interested you may take a look at ***Hadley Wickham's Advanced R Book*** (it is a free bookdown). I will be using the R6 system, as it is the easiest, and most similar to other modern systems in data science geared languages.\n\n# Class structure \n\nI created two classes in my OOP implementation of the decision tree classifier. One for the node and and one for the tree. The node is basic structure defining the main elements of node. That being its parent its childs, and the information the node contains (is it a decision or a split). Only the initialize method is defined for this class, it sets the default values to a node to null. The tree class contains all the helper functions as methods for creating the tree. It also contains the stopping conditions, the information metric to use, and the training data frame to use in its attributes. The root attribute, is where the fully grown tree is later stored to be used (saves computation time). Note that these two structures are not sub-classes or anything like that; the node class is merely used in one of the methods for the tree. Finally, the initialize method for the tree only assigns the root as NULL, with the rest being assigned their inputted values.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/classdiagram.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n# Revised and new helper functions\n\nNote that all of my functions have had small changes made to them. This small change is that I used the `$self` semantic of the R6 system for accessing and modifying attributes, as well as other methods. I will now discuss any major changes that I have made to my helper functions, and any new ones I created\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_information_gain =\n  function(parent, l_child, r_child, mode = self$mode){\n    #Get weights in each child\n    weight_l = nrow(l_child)/nrow(parent)\n    weight_r = nrow(r_child)/nrow(parent)\n    #Choose mode\n    if(mode == \"gini\"){\n    gain = self$get_gini_impurity(parent[, ncol(parent)]) - (weight_l*self$get_gini_impurity(l_child[, ncol(l_child)]) + weight_r*self$get_gini_impurity(r_child[, ncol(r_child)]))\n        } else {\n    gain = self$get_entropy(as.character(parent[, ncol(parent)])) - (weight_l*self$get_entropy(as.character(l_child[, ncol(l_child)])) + weight_r*self$get_entropy(as.character(r_child[, ncol(r_child)])))\n        }\n}\n```\n:::\n\n\n\nPreviously, I only used entropy as my metric for finding the best split. I then calculated the weighted entropy, and found the split that had the lowest weighted entropy. I have since converted my functions to use information gain, with an option to choose either entropy or gini-impurity. Now the best split is the one that maximizes information gain. This change is reflected in my helper which finds the best split.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuild_tree = \n  function(df = self$df, curr_depth = 0){\n  data = df\n  #Split until stopping condition are met\n  if(!any(self$check_purity(data), \n          nrow(data) < self$min_samples, \n          curr_depth == self$max_depth)){\n    #Keep splitting\n    #Get potential and best splits\n    potential_splits = self$get_potential_splits(data)\n    best_split = self$determine_best_split(data, potential_splits, mode = self$mode)\n    #Record best split value and feature\n    split_column = best_split[[1]]\n    split_value = best_split[[2]]\n    #Split by best combo and assign\n    data_split = self$split_data(data, split_column, split_value)\n    data_above = data_split[[1]]\n    data_below = data_split[[2]]\n    #Recursion occurs here\n    left_subtree = self$build_tree(df = data_below, curr_depth = curr_depth + 1)\n    right_subtree = self$build_tree(df = data_above, curr_depth = curr_depth + 1)\n    #Return decision node\n    return(Node$new(depth = curr_depth, left = left_subtree, right = right_subtree, feature = split_column, value = split_value))\n          } else {\n    #Stop splitting\n    #Compute leaf node\n    leaf.value = self$classify_data(data)\n    return(Node$new(value = leaf.value, depth = curr_depth))\n          }\n}\n```\n:::\n\n\n\nThis is by far the biggest change to my helpers from the v1 classifier. The idea behind the build tree is similar to what it was previously. In fact most of the code is the same. All that has changed is that when I call the function for recursion, I also create a new node using the new node class. This essentially just stores the nodes as the splits are being created, or when a stopping condition is met.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint_tree = \n  function(tree = self$build_tree(self$df)){\n    #Print decision node\n    if(!is.null(tree$feature)){\n    cat(tree$depth, paste0(paste(rep(\"\\t\", tree$depth), collapse = \"\"), tree$depth), tree$feature, \"<=\", tree$value, \"\\n\")}\n          #Check for nullity of left and right; leaf\n    if(!any(is.null(tree$left), is.null(tree$right))){\n      self$print_tree(tree = tree$left)\n      self$print_tree(tree = tree$right)\n    } else {\n      cat(paste0(tree$depth, paste(rep(\"\\t\", tree$depth), collapse = \"\"), tree$depth), \"predict:\", tree$value, \"\\n\")\n          }\n        }\n```\n:::\n\n\n\nThe build tree functions creates the tree. It is stored in a data type called an environment in R. The print tree function descends down this environment recursively, and prints the important attributes needed for creating a visualization of the tree. The stopping conditions for my tree were if a node had no children. It was also important to print each part of the tree on a new line, and for nodes at the same depth to have the same indent. This makes following the tree visually quite easy to follow (although I must say my printing function is by no means the best).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = \n  function(){\n    #Build the tree once for predictions only\n    self$root = self$build_tree()\n  }\n```\n:::\n\n\n\nThis method just builds the tree and stores it in an attribute of the tree class called root. Nothing special here. Storing the tree saves computation time later, when we have to make predictions on the testing data set. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_prediction = \n  function(y, tree = self$root){\n    if(class(tree$value) == \"character\") return(tree$value)\n      partition.val = tree$value\n      feature.num = which(names(self$df) == tree$feature)\n      if(y[[feature.num]] < partition.val){\n       self$make_prediction(y = y, tree = tree$left) \n      } else {\n        self$make_prediction(y = y, tree = tree$right)\n      }\n}\n```\n:::\n\n\n\nThe make prediction method takes a single row or observation and returns the predicted class. It works by descending down the tree we have build and stored recursively. It does this by looking converting each decision node into an if statement, and comparing the corresponding element of the new observation. The stopping condition here is if the value attribute is a character. In my class diagrams I should the value attribute could be numeric or character. The numeric case is for when the node is a decision node, and the character case is for when the node is a leaf node. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_predictions =\n  function(Y){\n    predict.dt = double(length = nrow(Y))\n    for(i in 1:nrow(Y)){\n      row.val = as.numeric(Y[i, 1:(ncol(Y)-1)])\n      predict.dt[i] = self$make_prediction(y = row.val)\n      next(i)\n    }\n    return(predict.dt)\n}\n```\n:::\n\n\n\nThe final get prediction method takes a testing data frame and makes a prediction on each sample by calling the make prediction method for a single sample. It then returns these predictions\n\n# Full OOP code\n\nHere is the full code for the decision tree classifier\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNode = \n  R6Class(\n    classname = \"Node\", \n    public = list(\n      depth = \"numeric\", \n      left = NULL,\n      right = NULL,\n      feature = NULL, \n      value = NULL, \n      initialize =\n        function(depth = NULL, left = NULL, right = NULL, feature = NULL, value = NULL){\n          self$depth = depth\n          self$left = left\n          self$right = right\n          self$feature = feature\n          self$value = value\n        })\n  )\n\nTree = \n  R6Class(\n    classname = \"Tree\",\n    public = list(\n      min_samples = \"numeric\", \n      max_depth = \"numeric\",\n      mode = \"character\",\n      df = \"data.frame\",\n      root = NULL,\n      initialize = \n        function(min_samples, max_depth, mode, df, root = NULL){\n          self$min_samples = min_samples\n          self$max_depth = max_depth\n          self$mode = mode\n          self$df = df\n          self$root = root\n        },\n      get_entropy =\n        function(x){\n          if(length(x) == 0) return(0)\n          weights = table(x)/length(x)\n          info_content = -weights*log2(weights)\n          entropy = sum(info_content)\n          return(entropy)\n        },\n      get_gini_impurity =\n        function(x){\n        #Assume x is a factor with labels\n        if(length(x) == 0) return(0)\n        weights = table(x)/length(x)\n        weights_squared = weights^2\n        sum_of_squares = sum(weights_squared)\n        gini = 1 - sum_of_squares\n        return(gini)\n      },\n      get_information_gain =\n        function(parent, l_child, r_child, mode = self$mode){\n        #Get weights in each child\n        weight_l = nrow(l_child)/nrow(parent)\n        weight_r = nrow(r_child)/nrow(parent)\n        #Choose mode\n        if(mode == \"gini\"){\n          gain = self$get_gini_impurity(parent[, ncol(parent)]) - (weight_l*self$get_gini_impurity(l_child[, ncol(l_child)]) + weight_r*self$get_gini_impurity(r_child[, ncol(r_child)]))\n        } else {\n          gain = self$get_entropy(as.character(parent[, ncol(parent)])) - (weight_l*self$get_entropy(as.character(l_child[, ncol(l_child)])) + weight_r*self$get_entropy(as.character(r_child[, ncol(r_child)])))\n        }\n      },\n      check_purity = \n        function(data){\n          #Get unique labels\n          labels = length(unique(data[, ncol(data)]))\n          #Check if there is only one\n          ifelse(labels == 1, return(TRUE), return(FALSE))\n        },\n      classify_data = \n        function(data){\n          #Get labels\n          get_labels = data[, ncol(data)]\n          #Get label frequency and max\n          label_freq = table(get_labels)\n          label_freq_a = as.data.frame(label_freq)\n          label_dom = max(label_freq)\n          #Get classification\n          for(i in 1:nrow(label_freq_a)){\n            if(label_freq_a$Freq[i] == label_dom){\n              classification = as.character(label_freq_a$get_labels[i])\n            } else {\n              next(i)\n            }\n          }\n          return(classification)\n        },\n      split_data =\n        function(data, split_column, split_value){\n          split_c = data[[split_column]]\n          #Filter the data into above and below\n          data_below = data[split_c <= split_value, ]\n          data_above = data[split_c > split_value, ]\n          return(list(data_above, data_below))\n        },\n      get_potential_splits =\n        function(data){\n          #Sorting stage\n          data = data\n          col_n = ncol(data) - 1\n          for(i in 1:col_n){\n            data_i = sort(data[, i])\n            data[, i] = data_i\n          }\n          #Creating the splits\n          dat = data[0, ]\n          for(j in 1:col_n){\n            for(i in 2:nrow(data)){\n              curr_val = data[i, j]\n              previous_val = data[(i-1), j]\n              potential_val = (curr_val + previous_val)/2\n              dat[(i-1), j] = potential_val\n            }\n          }\n          dat[nrow(dat)+1, ] = data[nrow(data), ]\n          dat = dat[, 1:col_n]\n          potential_splits = as.data.frame(dat)\n          if(ncol(potential_splits) == 1){\n            colnames(potential_splits)[[1]] = colnames(data)[[1]]\n          }\n          return(potential_splits)\n        },\n      determine_best_split =\n        function(data, potential_splits, mode = self$mode){\n        #Initialize information, feature, feature val\n        running_gain = -Inf\n        best_split_value = 0\n        best_split_column = \"\"\n        #Find best entropy over potential splits\n        for(j in 1:ncol(potential_splits)){\n          for(i in unique(potential_splits[, j])){\n            mask_val = i \n            mask_col = j\n            splits = self$split_data(data = data, split_column = mask_col, split_value = mask_val)\n            relative_gain = self$get_information_gain(parent = data, r_child = splits[[1]], l_child = splits[[2]], mode = mode)\n            if(relative_gain > running_gain){\n              running_gain = relative_gain\n              best_split_value = mask_val\n              best_split_column = colnames(potential_splits)[j]\n            } else {\n              next(i)\n            }\n          }\n        }\n        return(list(best_split_column, best_split_value))\n      },\n      build_tree = \n        function(df = self$df, curr_depth = 0){\n          data = df\n          #Split until stopping condition are met\n          if(!any(self$check_purity(data), \n                  nrow(data) < self$min_samples, \n                  curr_depth == self$max_depth)){\n            #Keep splitting\n            #Get potential and best splits\n            potential_splits = self$get_potential_splits(data)\n            best_split = self$determine_best_split(data, potential_splits, mode = self$mode)\n            #Record best split value and feature\n            split_column = best_split[[1]]\n            split_value = best_split[[2]]\n            #Split by best combo and assign\n            data_split = self$split_data(data, split_column, split_value)\n            data_above = data_split[[1]]\n            data_below = data_split[[2]]\n            #Recursion occurs here\n            left_subtree = self$build_tree(df = data_below, curr_depth = curr_depth + 1)\n            right_subtree = self$build_tree(df = data_above, curr_depth = curr_depth + 1)\n            #Return decision node\n            return(Node$new(depth = curr_depth, left = left_subtree, right = right_subtree, feature = split_column, value = split_value))\n          } else {\n            #Stop splitting\n            #Compute leaf node\n            leaf.value = self$classify_data(data)\n            return(Node$new(value = leaf.value, depth = curr_depth))\n          }\n        }, \n      print_tree =\n        function(tree = self$build_tree(self$df)){\n          #Print decision node\n          if(!is.null(tree$feature)){\n          cat(tree$depth, paste0(paste(rep(\"\\t\", tree$depth), collapse = \"\"), tree$depth), tree$feature, \"<=\", tree$value, \"\\n\")}\n          #Check for nullity of left and right; leaf\n          if(!any(is.null(tree$left), is.null(tree$right))){\n            self$print_tree(tree = tree$left)\n            self$print_tree(tree = tree$right)\n          } else {\n            cat(paste0(tree$depth, paste(rep(\"\\t\", tree$depth), collapse = \"\"), tree$depth), \"predict:\", tree$value, \"\\n\")\n          }\n        },\n      fit =\n        function(){\n          #Build the tree once for predictions only\n          self$root = self$build_tree()\n        },\n      make_prediction = \n        function(y, tree = self$root){\n          if(class(tree$value) == \"character\") return(tree$value)\n          partition.val = tree$value\n          feature.num = which(names(self$df) == tree$feature)\n          if(y[[feature.num]] < partition.val){\n           self$make_prediction(y = y, tree = tree$left) \n          } else {\n            self$make_prediction(y = y, tree = tree$right)\n          }\n        }, \n      get_predictions =\n        function(Y){\n          predict.dt = double(length = nrow(Y))\n          for(i in 1:nrow(Y)){\n            row.val = as.numeric(Y[i, 1:(ncol(Y)-1)])\n            predict.dt[i] = self$make_prediction(y = row.val)\n            next(i)\n          }\n          return(predict.dt)\n        }\n      \n      \n    ))\n```\n:::\n\n\n\n# Testing\n\n## Salmon-tuna dataset\n\nWe begin by first creating a train test split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2310)\nsalmon_fish_split = train_test_split(data = salmon_fish, test = 0.1)\nsalmon_fish_train = subset(salmon_fish_split, salmon_fish_split$my.folds == \"train\")[, -4]\nsalmon_fish_test = subset(salmon_fish_split, salmon_fish_split$my.folds == \"test\")[, -4]\n```\n:::\n\n\n\n\nWe first specify the parameters for our new tree by creating an instance of the `Tree` class\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtree = Tree$new(min_samples = 10, \n                 max_depth = 5, \n                 mode = \"entropy\", \n                 df = salmon_fish_train)\n```\n:::\n\n\n\nWe then can build the tree according to these parameters. Lets run the fit method as well in anticipation for making a prediction after. We will also print three\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Build tree for viewing\nbtree = dtree$build_tree()\n#Store for predictions\ndtree$fit()\n#Print the tree\ndtree$print_tree()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0 0 length <= 2.996015 \n1\t1 predict: tuna \n1 \t1 weight <= 4.006707 \n2\t\t2 predict: salmon \n2 \t\t2 length <= 6.978225 \n3 \t\t\t3 length <= 4.998481 \n4 \t\t\t\t4 weight <= 6.948427 \n5\t\t\t\t\t5 predict: tuna \n5\t\t\t\t\t5 predict: salmon \n4 \t\t\t\t4 length <= 6.017146 \n5\t\t\t\t\t5 predict: tuna \n5\t\t\t\t\t5 predict: salmon \n3\t\t\t3 predict: tuna \n```\n\n\n:::\n:::\n\n\n\nWe can make a prediction on a single value. If you remember the graph from my v1 version of the salmon-fish df, you can visually confirm that the function works correctly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtree$make_prediction(y = c(7.5, 2.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"salmon\"\n```\n\n\n:::\n:::\n\n\n\nWe can then also make a prediction on the training set as well and calculate the accuracy for this particular train test split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions = salmon_fish_test$type == dtree$get_predictions(Y = salmon_fish_test)\naccuracy = length(which(predictions == TRUE))/nrow(salmon_fish_test)\nprint(accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.94\n```\n\n\n:::\n:::\n\n\n\nThe accuracy for this particular split is 94%. \n\n## Iris dataset\n\nBelow is an example for the iris data set. I have changed the parameters of the tree as well this time. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Train test split\nset.seed(2310)\niris_split = train_test_split(data = iris, test = 0.1)\niris_train = subset(iris_split, iris_split$my.folds == \"train\")[, -6]\niris_test = subset(iris_split, iris_split$my.folds == \"test\")[, -6]\n\n#Create instance \nitree = Tree$new(min_samples = 7, \n                 max_depth = 7, \n                 mode = \"gini\", \n                 df = iris_train)\n\n#Build tree for viewing\njtree = dtree$build_tree()\n#Store for predictions\nitree$fit()\n#Print the tree\nitree$print_tree()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0 0 Petal.Length <= 1.9 \n1\t1 predict: setosa \n1 \t1 Petal.Width <= 1.7 \n2 \t\t2 Petal.Length <= 4.9 \n3 \t\t\t3 Petal.Width <= 1.65 \n4\t\t\t\t4 predict: versicolor \n4\t\t\t\t4 predict: virginica \n3\t\t\t3 predict: virginica \n2 \t\t2 Petal.Length <= 4.8 \n3\t\t\t3 predict: virginica \n3\t\t\t3 predict: virginica \n```\n\n\n:::\n\n```{.r .cell-code}\n#Accuracy on testing data\npredictions = iris_test$Species == itree$get_predictions(Y = iris_test)\naccuracy = length(which(predictions == TRUE))/nrow(iris_test)\nprint(accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n\nThe accuracy for this particular split is 100%.\n\n\n## Single variable example \n\nBelow, I would just like to highlight that this implementation works on data set with a single predictor only.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nztree = Tree$new(min_samples = 7, \n                 max_depth = 6, \n                 mode = \"entropy\", \n                 df = iris[, c(3,5)])\nltree = ztree$build_tree()\nztree$fit()\nztree$print_tree()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0 0 Petal.Length <= 1.9 \n1\t1 predict: setosa \n1 \t1 Petal.Length <= 4.7 \n2 \t\t2 Petal.Length <= 4.4 \n3\t\t\t3 predict: versicolor \n3 \t\t\t3 Petal.Length <= 4.5 \n4 \t\t\t\t4 Petal.Length <= 4.5 \n5 \t\t\t\t\t5 Petal.Length <= 4.5 \n6\t\t\t\t\t\t6 predict: versicolor \n6\t\t\t\t\t\t6 predict: virginica \n5\t\t\t\t\t5 predict: virginica \n4\t\t\t\t4 predict: versicolor \n2 \t\t2 Petal.Length <= 5.1 \n3 \t\t\t3 Petal.Length <= 4.9 \n4 \t\t\t\t4 Petal.Length <= 4.8 \n5\t\t\t\t\t5 predict: virginica \n5\t\t\t\t\t5 predict: virginica \n4 \t\t\t\t4 Petal.Length <= 5 \n5\t\t\t\t\t5 predict: virginica \n5 \t\t\t\t\t5 Petal.Length <= 5.1 \n6\t\t\t\t\t\t6 predict: virginica \n6\t\t\t\t\t\t6 predict: virginica \n3\t\t\t3 predict: virginica \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}